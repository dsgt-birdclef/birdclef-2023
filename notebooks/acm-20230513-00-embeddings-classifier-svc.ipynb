{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- species: string (nullable = true)\n",
      " |-- track_stem: string (nullable = true)\n",
      " |-- track_type: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- embedding: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- prediction_vec: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- predictions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- rank: long (nullable = true)\n",
      " |    |    |-- index: long (nullable = true)\n",
      " |    |    |-- label: string (nullable = true)\n",
      " |    |    |-- mapped_label: string (nullable = true)\n",
      " |    |    |-- probability: double (nullable = true)\n",
      " |-- start_time: long (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from birdclef.utils import get_spark\n",
    "\n",
    "spark = get_spark(cores=16, memory=\"20g\")\n",
    "df = spark.read.parquet(\n",
    "    \"../data/processed/birdclef-2023/train_embeddings/consolidated_v3\"\n",
    ")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+--------------------+----+-----+--------------------+------------+--------------------+\n",
      "|species|track_stem|track_type|start_time|          track_name|rank|index|               label|mapped_label|         probability|\n",
      "+-------+----------+----------+----------+--------------------+----+-----+--------------------+------------+--------------------+\n",
      "|abythr1|  XC233199|   source0|        15|abythr1/XC233199_...|   0| 3185|Turdus leucomelas...|     pabthr1|  0.2241482138633728|\n",
      "|abythr1|  XC233199|   source0|         6|abythr1/XC233199_...|   0| 3021|Terpsiphone virid...|     afpfly1|0.040495436638593674|\n",
      "|abythr1|  XC233199|   source0|        18|abythr1/XC233199_...|   0| 3164|Turdus abyssinicu...|     abythr1|0.040056031197309494|\n",
      "|abythr1|  XC233199|   source0|         9|abythr1/XC233199_...|   0| 1151|Erpornis zanthole...|     whbyuh1| 0.15738973021507263|\n",
      "|abythr1|  XC233199|   source0|        54|abythr1/XC233199_...|   0| 3185|Turdus leucomelas...|     pabthr1|  0.9267532229423523|\n",
      "+-------+----------+----------+----------+--------------------+----+-----+--------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window, functions as F\n",
    "\n",
    "# keep the track_type for the highest energy\n",
    "highest_energy_channel = (\n",
    "    df\n",
    "    # get the track stem without the part\n",
    "    .withColumn(\"original_track_stem\", F.split(F.col(\"track_stem\"), \"_\").getItem(0))\n",
    "    .where(\"track_type != 'original'\")\n",
    "    # get the track type that has the most energy\n",
    "    .withColumn(\n",
    "        \"rank\",\n",
    "        F.rank().over(\n",
    "            Window.partitionBy(\"original_track_stem\").orderBy(F.desc(\"energy\"))\n",
    "        ),\n",
    "    )\n",
    "    # keep the first row\n",
    "    .where(F.col(\"rank\") == 1)\n",
    "    # drop the rank column\n",
    "    .select(\"species\", \"track_stem\", \"track_type\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# get the highest predictions by exploding the values\n",
    "exploded_embeddings = (\n",
    "    df\n",
    "    # join against the highest energy channel\n",
    "    .join(\n",
    "        highest_energy_channel,\n",
    "        on=[\"species\", \"track_stem\", \"track_type\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    # explode the embeddings, these are ordered by confidence\n",
    "    .withColumn(\"predictions\", F.explode(\"predictions\")).select(\n",
    "        \"species\",\n",
    "        \"track_stem\",\n",
    "        \"track_type\",\n",
    "        \"start_time\",\n",
    "        \"track_name\",\n",
    "        \"embedding\",\n",
    "        \"predictions.*\",\n",
    "    )\n",
    "    # simplifying assumption: we assume the prediction with the highest confidence is the true label\n",
    "    .where(\"rank = 0\")\n",
    ").cache()\n",
    "\n",
    "exploded_embeddings.drop(\"embedding\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|species|    n|\n",
      "+-------+-----+\n",
      "|thrnig1|12987|\n",
      "| wlwwar| 9249|\n",
      "|combuz1| 7173|\n",
      "| hoopoe| 6731|\n",
      "| barswa| 6191|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+---+\n",
      "|species|  n|\n",
      "+-------+---+\n",
      "|afpkin1|  3|\n",
      "|whhsaw1|  4|\n",
      "|whctur2|  4|\n",
      "|golher1|  5|\n",
      "|lotlap1|  8|\n",
      "+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# quick count of the number of samples\n",
    "counts = (\n",
    "    exploded_embeddings.groupBy(\"species\")\n",
    "    .agg(F.count(\"*\").alias(\"n\"))\n",
    "    .orderBy(F.desc(\"n\"))\n",
    ")\n",
    "counts.show(n=5)\n",
    "counts.orderBy(\"n\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|species|  n|\n",
      "+-------+---+\n",
      "|purgre2| 60|\n",
      "|bubwar2| 90|\n",
      "|rehwea1| 69|\n",
      "|kvbsun1| 80|\n",
      "|equaka1| 63|\n",
      "+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rarity_min_count = 100\n",
    "rare_species_count = (\n",
    "    exploded_embeddings.groupBy(\"species\")\n",
    "    .agg(F.count(\"*\").alias(\"n\"))\n",
    "    .where(f\"n < {rarity_min_count}\")\n",
    ")\n",
    "rare_species_count.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+\n",
      "|species|       probability|           embedding|\n",
      "+-------+------------------+--------------------+\n",
      "|afghor1| 0.511886715888977|[1.00166213512420...|\n",
      "|afghor1|0.9965255856513977|[0.57833033800125...|\n",
      "|afghor1|0.9904100894927979|[0.61484068632125...|\n",
      "|afghor1|0.9988522529602051|[1.26016914844512...|\n",
      "|afghor1|0.5952618718147278|[0.75000149011611...|\n",
      "+-------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74490"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if there are a lot of examples, we can use a higher threshold\n",
    "common_species = exploded_embeddings.where(\"probability > 0.4\").join(\n",
    "    rare_species_count.select(\"species\"), on=\"species\", how=\"left_anti\"\n",
    ")\n",
    "# these ones are less common so we use a lower threshold so we have at least one\n",
    "# example for each species\n",
    "rare_species = exploded_embeddings.where(\"probability > 0.05\").join(\n",
    "    rare_species_count.select(\"species\"), on=\"species\", how=\"inner\"\n",
    ")\n",
    "prepared = common_species.union(rare_species).select(\n",
    "    \"species\", \"probability\", \"embedding\"\n",
    ")\n",
    "prepared.show(n=5)\n",
    "prepared.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"species\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of species 264\n",
      "+-------+----+\n",
      "|species|   n|\n",
      "+-------+----+\n",
      "|thrnig1|3833|\n",
      "| hoopoe|3822|\n",
      "|eubeat1|3116|\n",
      "| wlwwar|2687|\n",
      "| barswa|2603|\n",
      "+-------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+---+\n",
      "|species|  n|\n",
      "+-------+---+\n",
      "|afpkin1|  2|\n",
      "|whctur2|  2|\n",
      "|rehblu1|  2|\n",
      "|whhsaw1|  3|\n",
      "|easmog1|  4|\n",
      "+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets check that we have the right number of classes, and how many examples we are working with\n",
    "prepared_counts = (\n",
    "    prepared.groupBy(\"species\").agg(F.count(\"*\").alias(\"n\")).orderBy(F.desc(\"n\"))\n",
    ")\n",
    "print(f\"number of species {prepared_counts.count()}\")\n",
    "\n",
    "prepared_counts.show(n=5)\n",
    "prepared_counts.orderBy(\"n\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.7261046603646"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of clips that have a prediction of any kind\n",
    "prepared.count() / exploded_embeddings.count() * 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "base classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>probability</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afghor1</td>\n",
       "      <td>0.511887</td>\n",
       "      <td>[1.0016621351242065, 1.2551445960998535, 0.242...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afghor1</td>\n",
       "      <td>0.996526</td>\n",
       "      <td>[0.5783303380012512, 1.845029354095459, 0.2178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afghor1</td>\n",
       "      <td>0.990410</td>\n",
       "      <td>[0.6148406863212585, 1.5936590433120728, 0.504...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afghor1</td>\n",
       "      <td>0.998852</td>\n",
       "      <td>[1.2601691484451294, 2.366661787033081, 0.2103...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afghor1</td>\n",
       "      <td>0.595262</td>\n",
       "      <td>[0.7500014901161194, 1.3813127279281616, 0.219...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species  probability                                          embedding\n",
       "0  afghor1     0.511887  [1.0016621351242065, 1.2551445960998535, 0.242...\n",
       "1  afghor1     0.996526  [0.5783303380012512, 1.845029354095459, 0.2178...\n",
       "2  afghor1     0.990410  [0.6148406863212585, 1.5936590433120728, 0.504...\n",
       "3  afghor1     0.998852  [1.2601691484451294, 2.366661787033081, 0.2103...\n",
       "4  afghor1     0.595262  [0.7500014901161194, 1.3813127279281616, 0.219..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = prepared.toPandas()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "\n",
    "def model_eval(truth, preds):\n",
    "    print(\"Accuracy:\", accuracy_score(truth, preds))\n",
    "    print(\n",
    "        \"Precision:\",\n",
    "        precision_score(truth, preds, average=\"macro\"),\n",
    "    )\n",
    "    print(\n",
    "        \"Recall:\",\n",
    "        recall_score(truth, preds, average=\"macro\"),\n",
    "    )\n",
    "    print(\n",
    "        \"F1 Score:\",\n",
    "        f1_score(truth, preds, average=\"macro\"),\n",
    "    )\n",
    "\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(\n",
    "    np.stack(data[\"embedding\"]),\n",
    "    data[\"species\"],\n",
    "    test_size=0.33,\n",
    "    stratify=data[\"species\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc\n",
      "CPU times: total: 11min 50s\n",
      "Wall time: 11min 54s\n",
      "Accuracy: 0.9250264421121146\n",
      "Precision: 0.6870300689036098\n",
      "Recall: 0.8614117101411818\n",
      "F1 Score: 0.7402244935237366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\kaggle\\birdclef-2023\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# check how long it takes to train linearsvc vs svc\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "# NOTE: this doesnt have an option to dump out probabilities\n",
    "# print(\"linearsvc\")\n",
    "# %time clf = LinearSVC().fit(train_x, train_y)\n",
    "# model_eval(clf.predict(test_x), test_y)\n",
    "\n",
    "# CPU times: total: 2min 15s\n",
    "# Wall time: 2min 15s\n",
    "# Accuracy: 0.9044829550077292\n",
    "# Precision: 0.7786332701592122\n",
    "# Recall: 0.7993822579967949\n",
    "# F1 Score: 0.7792748609128219\n",
    "\n",
    "# setting probabilities makes this _much_ slower than it needs to be\n",
    "print(\"svc\")\n",
    "%time clf = SVC(probability=True).fit(train_x, train_y)\n",
    "%time model_eval(clf.predict(test_x), test_y)\n",
    "\n",
    "# without pca\n",
    "# CPU times: total: 11min 50s\n",
    "# Wall time: 11min 54s\n",
    "# Accuracy: 0.9250264421121146\n",
    "# Precision: 0.6870300689036098\n",
    "# Recall: 0.8614117101411818\n",
    "# F1 Score: 0.7402244935237366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc with pca\n",
      "CPU times: total: 9min 32s\n",
      "Wall time: 10min 22s\n"
     ]
    }
   ],
   "source": [
    "# use a pipeline to setup pca\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"svc with pca\")\n",
    "%time clf = Pipeline([(\"pca\", PCA(n_components=0.90)), (\"svc\", SVC(probability=True))]).fit(train_x, train_y)\n",
    "%time model_eval(clf.predict(test_x), test_y)\n",
    "\n",
    "# svc with pca\n",
    "# CPU times: total: 9min 32s\n",
    "# Wall time: 10min 22s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression\n",
      "CPU times: total: 29min 35s\n",
      "Wall time: 4min 21s\n",
      "Accuracy: 0.9232365145228216\n",
      "Precision: 0.7666584388212514\n",
      "Recall: 0.8696996457554622\n",
      "F1 Score: 0.8028503796054116\n",
      "CPU times: total: 531 ms\n",
      "Wall time: 1.99 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\kaggle\\birdclef-2023\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print(\"logistic regression\")\n",
    "%time clf = LogisticRegression(solver=\"newton-cholesky\").fit(train_x, train_y)\n",
    "%time model_eval(clf.predict(test_x), test_y)\n",
    "\n",
    "# CPU times: total: 29min 35s\n",
    "# Wall time: 4min 21s\n",
    "# Accuracy: 0.9232365145228216\n",
    "# Precision: 0.7666584388212514\n",
    "# Recall: 0.8696996457554622\n",
    "# F1 Score: 0.8028503796054116\n",
    "# CPU times: total: 531 ms\n",
    "# Wall time: 1.99 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204, 320)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.steps[0][1].components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression with pca\n",
      "CPU times: total: 17min 17s\n",
      "Wall time: 3min 5s\n",
      "Accuracy: 0.9201855015865267\n",
      "Precision: 0.7663412257014655\n",
      "Recall: 0.8615520243852796\n",
      "F1 Score: 0.7988655932230133\n",
      "CPU times: total: 1.03 s\n",
      "Wall time: 397 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\kaggle\\birdclef-2023\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"logistic regression with pca\")\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"pca\", PCA(n_components=0.95)),\n",
    "        (\"lr\", LogisticRegression(solver=\"newton-cholesky\")),\n",
    "    ]\n",
    ")\n",
    "%time clf = pipeline.fit(train_x, train_y)\n",
    "%time model_eval(clf.predict(test_x), test_y)\n",
    "\n",
    "# logistic regression with pca 0.90\n",
    "# CPU times: total: 13min 16s\n",
    "# Wall time: 2min 6s\n",
    "# Accuracy: 0.9162802050280693\n",
    "# Precision: 0.7555922870382019\n",
    "# Recall: 0.8559744316050081\n",
    "# F1 Score: 0.7887907714809498\n",
    "# dims = (204, 320)\n",
    "\n",
    "# logistic regression with pca 0.95\n",
    "# CPU times: total: 17min 17s\n",
    "# Wall time: 3min 5s\n",
    "# Accuracy: 0.9201855015865267\n",
    "# Precision: 0.7663412257014655\n",
    "# Recall: 0.8615520243852796\n",
    "# F1 Score: 0.7988655932230133\n",
    "# CPU times: total: 1.03 s\n",
    "# Wall time: 397 ms\n",
    "# dims = (250, 320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 320)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.steps[0][1].components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic regression with pca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\kaggle\\birdclef-2023\\venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 14min 58s\n",
      "Wall time: 15min 9s\n",
      "Accuracy: 0.918192173134814\n",
      "Precision: 0.753662741892275\n",
      "Recall: 0.8684001488280116\n",
      "F1 Score: 0.7937295973057623\n",
      "CPU times: total: 641 ms\n",
      "Wall time: 366 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\kaggle\\birdclef-2023\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# create a pipeline where we feature scale, then pca, then train\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"logistic regression with pca\")\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scale\", StandardScaler()),\n",
    "        # (\"pca\", PCA(n_components=0.95)),\n",
    "        (\"lr\", LogisticRegression(solver=\"saga\")),\n",
    "    ]\n",
    ")\n",
    "%time clf = pipeline.fit(train_x, train_y)\n",
    "%time model_eval(clf.predict(test_x), test_y)\n",
    "\n",
    "# CPU times: total: 14min 58s\n",
    "# Wall time: 15min 9s\n",
    "# Accuracy: 0.918192173134814\n",
    "# Precision: 0.753662741892275\n",
    "# Recall: 0.8684001488280116\n",
    "# F1 Score: 0.7937295973057623\n",
    "# CPU times: total: 641 ms\n",
    "# Wall time: 366 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\kaggle\\birdclef-2023\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\kaggle\\birdclef-2023\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\kaggle\\birdclef-2023\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\kaggle\\birdclef-2023\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('C', 414521.9791438899), ('gamma', 7.245553401489778e-06), ('kernel', 'rbf')])\n",
      "Accuracy: 0.9370677731673582\n",
      "Precision: 0.8922478213125218\n",
      "Recall: 0.8216002023315834\n",
      "F1 Score: 0.8442900853568304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\kaggle\\birdclef-2023\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "cv_model = BayesSearchCV(\n",
    "    SVC(probability=True),\n",
    "    {\n",
    "        \"C\": (1e-6, 1e6, \"log-uniform\"),\n",
    "        \"gamma\": (1e-6, 1e1, \"log-uniform\"),\n",
    "        \"kernel\": [\"rbf\", \"linear\"],\n",
    "    },\n",
    "    n_iter=32,\n",
    "    scoring=\"precision_macro\",\n",
    "    verbose=4,\n",
    "    cv=3,\n",
    "    n_points=8,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "cv_model.fit(train_x, train_y)\n",
    "print(cv_model.best_params_)\n",
    "model_eval(test_y, cv_model.predict(test_x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving models to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "pickle.dump(\n",
    "    cv_model,\n",
    "    Path(\"../data/models/baseline/svc-opt-v1.pkl\").open(\"wb\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
