{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- species: string (nullable = true)\n",
      " |-- track_stem: string (nullable = true)\n",
      " |-- track_type: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- embedding: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- prediction_vec: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- predictions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- rank: long (nullable = true)\n",
      " |    |    |-- index: long (nullable = true)\n",
      " |    |    |-- label: string (nullable = true)\n",
      " |    |    |-- mapped_label: string (nullable = true)\n",
      " |    |    |-- probability: double (nullable = true)\n",
      " |-- start_time: long (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- start_time: long (nullable = true)\n",
      " |-- prediction: string (nullable = true)\n",
      " |-- probability: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- primary_label: string (nullable = true)\n",
      " |-- secondary_labels: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- scientific_name: string (nullable = true)\n",
      " |-- common_name: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from birdclef.utils import get_spark\n",
    "from pyspark.sql import Window, functions as F\n",
    "import os\n",
    "\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = \"../data/tmp/spark\"\n",
    "\n",
    "spark = get_spark(cores=24, memory=\"30g\")\n",
    "df = spark.read.parquet(\n",
    "    \"../data/processed/birdclef-2023/train_embeddings/consolidated_v3\"\n",
    ")\n",
    "df.printSchema()\n",
    "\n",
    "preds = spark.read.parquet(\"../data/processed/birdclef-2023/consolidated_v3_with_preds\")\n",
    "preds.printSchema()\n",
    "\n",
    "# also include the metadata\n",
    "birdclef_root = \"../data/raw/birdclef-2023\"\n",
    "train_metadata = spark.read.csv(f\"{birdclef_root}/train_metadata.csv\", header=True)\n",
    "train_metadata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_indices = [\n",
    "    (1022, \"Dog_Dog\"),\n",
    "    (1136, \"Engine_Engine\"),\n",
    "    (1141, \"Environmental_Environmental\"),\n",
    "    (1219, \"Fireworks_Fireworks\"),\n",
    "    (1352, \"Gun_Gun\"),\n",
    "    (1449, \"Human non-vocal_Human non-vocal\"),\n",
    "    (1450, \"Human vocal_Human vocal\"),\n",
    "    (1451, \"Human whistle_Human whistle\"),\n",
    "    (1997, \"Noise_Noise\"),\n",
    "    (2812, \"Siren_Siren\"),\n",
    "]\n",
    "\n",
    "\n",
    "def keep_top_n(df, n=250):\n",
    "    return (\n",
    "        df.withColumn(\n",
    "            \"rank\",\n",
    "            F.row_number().over(\n",
    "                Window.partitionBy(\"index\").orderBy(F.desc(\"probability\"))\n",
    "            ),\n",
    "        )\n",
    "        .where(f\"rank <= {n}\")\n",
    "        .select(\n",
    "            \"track_name\", \"start_time\", F.lit(\"no_call\").alias(\"species\"), \"embedding\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "exploded_noise = (\n",
    "    df\n",
    "    # explode the predictions with their indices\n",
    "    .select(\n",
    "        \"track_name\",\n",
    "        \"start_time\",\n",
    "        \"embedding\",\n",
    "        F.posexplode(\"prediction_vec\").alias(\"index\", \"logit\"),\n",
    "    )\n",
    "    .where(F.col(\"index\").isin([i[0] for i in noise_indices]))\n",
    "    .withColumn(\"probability\", F.expr(\"1/(1+exp(-logit))\"))\n",
    ").cache()\n",
    "\n",
    "\n",
    "negative_samples = keep_top_n(exploded_noise).cache()\n",
    "\n",
    "# environmental and noise, to average with the other embeddings\n",
    "noise_samples = keep_top_n(\n",
    "    exploded_noise.where(F.col(\"index\").isin([1141, 1997])), n=50\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\kaggle\\birdclef-2023\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@F.udf(returnType=\"array<string>\")\n",
    "def parse_labels(label_str: str):\n",
    "    # use literal eval to parse the string\n",
    "    return ast.literal_eval(label_str)\n",
    "\n",
    "\n",
    "@F.pandas_udf(\"array<float>\", F.PandasUDFType.GROUPED_AGG)\n",
    "def embedding_mean(v):\n",
    "    return np.stack(v).mean(axis=0).tolist()\n",
    "\n",
    "\n",
    "# whats the lowest number tracks for a species?\n",
    "primary_labels = train_metadata.select(\n",
    "    F.col(\"primary_label\").alias(\"species\"), \"filename\"\n",
    ")\n",
    "# now find all the secondary labels and see how frequent they can be\n",
    "secondary_labels = train_metadata.select(\n",
    "    F.explode(parse_labels(\"secondary_labels\")).alias(\"species\"), \"filename\"\n",
    ")\n",
    "\n",
    "multi_label = (\n",
    "    primary_labels.union(secondary_labels)\n",
    "    .distinct()\n",
    "    .groupBy(\"filename\")\n",
    "    .agg(F.collect_list(\"species\").alias(\"species\"))\n",
    "    .where(F.size(\"species\") > 1)\n",
    "    # extract the xeno-canto id with regex\n",
    "    .withColumn(\"xc_id\", F.regexp_extract(F.col(\"filename\"), r\"XC(\\d+)\", 1))\n",
    ")\n",
    "\n",
    "track_averaged_samples = (\n",
    "    df.withColumn(\"xc_id\", F.regexp_extract(F.col(\"track_name\"), r\"XC(\\d+)\", 1))\n",
    "    .drop(\"species\")\n",
    "    .join(multi_label, on=\"xc_id\")\n",
    "    .where(\"track_type = 'original'\")\n",
    "    .groupBy(\"species\")\n",
    "    .agg(embedding_mean(\"embedding\").alias(\"embedding\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the track name that has the most predictions with the main class\n",
    "most_representative_source = (\n",
    "    preds.groupBy(\"track_name\", \"prediction\")\n",
    "    .count()\n",
    "    .orderBy(\"track_name\", F.desc(\"count\"))\n",
    "    .withColumn(\"species\", F.split(\"track_name\", \"/\").getItem(0))\n",
    "    .where(\"track_name like '%source%'\")\n",
    "    .withColumn(\n",
    "        \"track_stem\", F.split(F.split(\"track_name\", \"/\").getItem(1), \"_\").getItem(0)\n",
    "    )\n",
    "    .where(\"species = prediction\")\n",
    "    .withColumn(\n",
    "        \"rank\",\n",
    "        F.row_number().over(Window.partitionBy(\"track_stem\").orderBy(F.desc(\"count\"))),\n",
    "    )\n",
    "    .where(\"rank = 1\")\n",
    "    .select(\"track_name\", \"prediction\", \"count\")\n",
    ")\n",
    "\n",
    "most_representative_embeddings = preds.join(\n",
    "    most_representative_source.select(\"track_name\", \"prediction\"),\n",
    "    on=[\"track_name\", \"prediction\"],\n",
    ")\n",
    "\n",
    "representative_samples = (\n",
    "    df\n",
    "    # join against the most representative source, and only keep the embeddings\n",
    "    # that are actually labeled by the previous model\n",
    "    .join(\n",
    "        most_representative_embeddings.select(\"track_name\", \"start_time\", \"prediction\"),\n",
    "        on=[\"track_name\", \"start_time\"],\n",
    "        how=\"inner\",\n",
    "    ).select(\"track_name\", \"start_time\", \"species\", \"embedding\", \"prediction\")\n",
    ").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each of the representative samples, generate a new sample by averaging it with a\n",
    "# random noise sample\n",
    "\n",
    "\n",
    "@F.pandas_udf(\"array<float>\", F.PandasUDFType.SCALAR)\n",
    "def embedding_pair_mean(v1, v2):\n",
    "    return (v1 + v2) / 2\n",
    "\n",
    "\n",
    "@F.pandas_udf(\"array<float>\", F.PandasUDFType.SCALAR)\n",
    "def embedding_triple_mean(v1, v2, v3):\n",
    "    return (v1 + v2 + v3) / 3\n",
    "\n",
    "\n",
    "best_representative_sample_per_track_with_noise = (\n",
    "    # generate a subset of rows, and then join against the noise samples\n",
    "    representative_samples.withColumn(\n",
    "        \"rank\",\n",
    "        F.row_number().over(\n",
    "            Window.partitionBy(\"track_name\").orderBy(F.desc(\"prediction\"))\n",
    "        ),\n",
    "    )\n",
    "    .where(\"rank = 1\")\n",
    "    .drop(\"rank\")\n",
    "    .withColumn(\"random_rank\", F.rand())\n",
    "    .withColumn(\n",
    "        \"rank\",\n",
    "        F.row_number().over(\n",
    "            Window.partitionBy(\"species\").orderBy(F.desc(\"random_rank\"))\n",
    "        ),\n",
    "    )\n",
    "    .where(\"rank <= 100\")\n",
    "    .drop(\"rank\", \"random_rank\")\n",
    "    # cross join against the noise samples\n",
    "    .join(noise_samples.selectExpr(\"embedding as noise_embedding\"), how=\"cross\")\n",
    "    # now there should be at least 1000 samples per class\n",
    "    .withColumn(\"random_rank\", F.rand())\n",
    "    .withColumn(\n",
    "        \"rank\",\n",
    "        F.row_number().over(Window.partitionBy(\"species\").orderBy(\"random_rank\")),\n",
    "    )\n",
    "    # 250k samples in total, this is a tad much for actual training so make sure\n",
    "    # to subsample\n",
    "    .where(\"rank <= 1000\")\n",
    "    .withColumn(\"embedding\", embedding_pair_mean(\"embedding\", \"noise_embedding\"))\n",
    "    .withColumn(\"index\", F.row_number().over(Window.orderBy(F.desc(\"random_rank\"))))\n",
    "    .select(F.array(F.col(\"species\")).alias(\"species\"), \"embedding\", \"rank\", \"index\")\n",
    ").cache()\n",
    "\n",
    "n_classes = primary_labels.select(\"species\").distinct().count()\n",
    "\n",
    "single_label_samples = (\n",
    "    # ~62k samples\n",
    "    best_representative_sample_per_track_with_noise.where(\"rank <= 250\").select(\n",
    "        \"species\", \"embedding\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# rough proportion of co-occurs, pairs and up\n",
    "# 1, 1/3, 1/10\n",
    "\n",
    "pair_label_samples = (\n",
    "    # we generate 100 per class for 25k samples\n",
    "    best_representative_sample_per_track_with_noise.withColumn(\"random_rank\", F.rand())\n",
    "    .withColumn(\"random_index\", (F.rand() * 1e6).cast(\"int\") % (n_classes * 500))\n",
    "    .withColumn(\n",
    "        \"rank\",\n",
    "        F.row_number().over(\n",
    "            Window.partitionBy(\"species\").orderBy(F.desc(\"random_rank\"))\n",
    "        ),\n",
    "    )\n",
    "    .where(\"rank <= 100\")\n",
    "    .join(\n",
    "        best_representative_sample_per_track_with_noise.selectExpr(\n",
    "            \"species as other_species\",\n",
    "            \"embedding as other_embedding\",\n",
    "            \"index as random_index\",\n",
    "        ),\n",
    "        on=\"random_index\",\n",
    "    )\n",
    "    .select(\n",
    "        F.array_union(F.col(\"species\"), F.col(\"other_species\")).alias(\"species\"),\n",
    "        embedding_pair_mean(\"embedding\", \"other_embedding\").alias(\"embedding\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# NOTE: this could surely be written better, but eh\n",
    "triple_label_samples = (\n",
    "    # we generate 30 per class for about 7.5k samples\n",
    "    best_representative_sample_per_track_with_noise.withColumn(\"random_rank\", F.rand())\n",
    "    .withColumn(\"random_index_1\", (F.rand() * 1e6).cast(\"int\") % (n_classes * 1000))\n",
    "    .withColumn(\"random_index_2\", (F.rand() * 1e6).cast(\"int\") % (n_classes * 1000))\n",
    "    .withColumn(\n",
    "        \"rank\",\n",
    "        F.row_number().over(\n",
    "            Window.partitionBy(\"species\").orderBy(F.desc(\"random_rank\"))\n",
    "        ),\n",
    "    )\n",
    "    .where(\"rank <= 30\")\n",
    "    .join(\n",
    "        best_representative_sample_per_track_with_noise.selectExpr(\n",
    "            \"species as species_1\",\n",
    "            \"embedding as embedding_1\",\n",
    "            \"index as random_index_1\",\n",
    "        ),\n",
    "        on=\"random_index_1\",\n",
    "    )\n",
    "    .join(\n",
    "        best_representative_sample_per_track_with_noise.selectExpr(\n",
    "            \"species as species_2\",\n",
    "            \"embedding as embedding_2\",\n",
    "            \"index as random_index_2\",\n",
    "        ),\n",
    "        on=\"random_index_2\",\n",
    "    )\n",
    "    .select(\n",
    "        F.array_union(\n",
    "            F.array_union(F.col(\"species\"), F.col(\"species_1\")), F.col(\"species_2\")\n",
    "        ).alias(\"species\"),\n",
    "        embedding_triple_mean(\"embedding\", \"embedding_1\", \"embedding_2\").alias(\n",
    "            \"embedding\"\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = (\n",
    "    track_averaged_samples.union(\n",
    "        representative_samples.select(F.array(\"species\").alias(\"species\"), \"embedding\")\n",
    "    )\n",
    "    .union(negative_samples.select(F.array(\"species\").alias(\"species\"), \"embedding\"))\n",
    "    .union(single_label_samples)\n",
    "    .union(pair_label_samples)\n",
    "    .union(triple_label_samples)\n",
    "    .select(\"species\", \"embedding\")\n",
    ")\n",
    "\n",
    "# let's write this to a parquet file, because we'll probably want to load this\n",
    "# a few times, plus its good to version\n",
    "\n",
    "train_df.write.parquet(\n",
    "    \"../data/processed/birdclef-2023/train_postprocessed/v1\", mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|  species|           embedding|\n",
      "+---------+--------------------+\n",
      "|[gobsta5]|[0.61802852153778...|\n",
      "|[chespa1]|[0.61116945743560...|\n",
      "|[golher1]|[0.90223264694213...|\n",
      "|[marsto1]|[0.61373400688171...|\n",
      "|[gobwea1]|[0.62747323513031...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "239569"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = spark.read.parquet(\"../data/processed/birdclef-2023/train_postprocessed/v1\")\n",
    "train_df.show(n=5)\n",
    "train_df.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
