{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- species: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- embedding: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      "\n",
      "+---------+--------------------+\n",
      "|  species|           embedding|\n",
      "+---------+--------------------+\n",
      "|[gobsta5]|[0.61802852153778...|\n",
      "|[chespa1]|[0.61116945743560...|\n",
      "|[golher1]|[0.90223264694213...|\n",
      "|[marsto1]|[0.61373400688171...|\n",
      "|[gobwea1]|[0.62747323513031...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(239569, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from birdclef.utils import get_spark\n",
    "import os\n",
    "\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = \"../data/tmp/spark\"\n",
    "\n",
    "spark = get_spark(cores=16, memory=\"2g\")\n",
    "df = spark.read.parquet(\"../data/processed/birdclef-2023/train_postprocessed/v1\")\n",
    "df.printSchema()\n",
    "df.show(n=5)\n",
    "\n",
    "data = df.toPandas()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239569, 265)\n",
      "(239569, 320)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(data[\"species\"])\n",
    "print(labels.shape)\n",
    "\n",
    "embeddings = np.stack(data[\"embedding\"])\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "\n",
    "def model_eval(truth, preds):\n",
    "    print(\"Accuracy:\", accuracy_score(truth, preds))\n",
    "    print(\n",
    "        \"Precision:\",\n",
    "        precision_score(truth, preds, average=\"macro\"),\n",
    "    )\n",
    "    print(\n",
    "        \"Recall:\",\n",
    "        recall_score(truth, preds, average=\"macro\"),\n",
    "    )\n",
    "    print(\n",
    "        \"F1 Score:\",\n",
    "        f1_score(truth, preds, average=\"macro\"),\n",
    "    )\n",
    "\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(embeddings, labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# weights = class_weight.compute_sample_weight(\n",
    "#     class_weight='balanced',\n",
    "#     y=test_y\n",
    "# )\n",
    "\n",
    "# clf = XGBClassifier(tree_method=\"gpu_hist\")\n",
    "# %time clf.fit(train_x, train_y, sample_weight=weights)\n",
    "\n",
    "# CPU times: total: 7min 20s\n",
    "# Wall time: 6min 53s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = clf.predict(test_x)\n",
    "# print(preds.shape)\n",
    "# print(test_y.shape)\n",
    "# model_eval(test_y, preds)\n",
    "# print(average_precision_score(test_y, clf.predict_proba(test_x)))\n",
    "\n",
    "# (47914, 265)\n",
    "# (47914, 265)\n",
    "# Accuracy: 0.7581708895103727\n",
    "# Precision: 0.9899858454212693\n",
    "# Recall: 0.7119283478314029\n",
    "# F1 Score: 0.8243448902217967\n",
    "# 0.9465624875416011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "\n",
    "weights = class_weight.compute_sample_weight(class_weight=\"balanced\", y=train_y)\n",
    "\n",
    "search = BayesSearchCV(\n",
    "    XGBClassifier(tree_method=\"gpu_hist\"),\n",
    "    {\n",
    "        \"eta\": (0.01, 1.0, \"log-uniform\"),\n",
    "        \"max_depth\": (1, 30, \"uniform\"),\n",
    "        \"gamma\": (0, 1, \"uniform\"),\n",
    "        \"min_child_weight\": (1, 10, \"uniform\"),\n",
    "    },\n",
    "    n_iter=10,\n",
    "    scoring=\"f1_macro\",\n",
    "    verbose=4,\n",
    "    cv=3,\n",
    "    n_points=1,\n",
    "    n_jobs=1,\n",
    ")\n",
    "%time search.fit(train_x, train_y, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7677714238009767\n",
      "Precision: 0.9866059859940541\n",
      "Recall: 0.7278978653841502\n",
      "F1 Score: 0.8345197010695892\n",
      "0.9432796210139014\n"
     ]
    }
   ],
   "source": [
    "model_eval(test_y, search.predict(test_x))\n",
    "print(average_precision_score(test_y, search.predict_proba(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('gamma', 0), ('max_depth', 6), ('min_child_weight', 5)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "pickle.dump(\n",
    "    clf,\n",
    "    Path(\"../data/models/baseline/xgbc-postprocess-best-acm-v1.pkl\").open(\"wb\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
