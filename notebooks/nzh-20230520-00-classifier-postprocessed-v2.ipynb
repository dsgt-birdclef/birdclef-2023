{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- track_stem: string (nullable = true)\n",
      " |-- start_time: long (nullable = true)\n",
      " |-- metadata_species: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- predicted_species: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- predicted_species_prob: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- embedding: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- next_embedding: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- track_embedding: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- primary_label: string (nullable = true)\n",
      " |-- species: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- species_count: long (nullable = false)\n",
      "\n",
      "+----------+----------+----------------+--------------------+----------------------+--------------------+--------------------+--------------------+-------------+--------------------+-------------+\n",
      "|track_stem|start_time|metadata_species|   predicted_species|predicted_species_prob|           embedding|      next_embedding|     track_embedding|primary_label|             species|species_count|\n",
      "+----------+----------+----------------+--------------------+----------------------+--------------------+--------------------+--------------------+-------------+--------------------+-------------+\n",
      "|  XC125862|        40|       [abythr1]|  [abythr1, wbrcha2]|  [0.72823923447200...|[1.4855427, 0.454...|[0.9370399, 0.233...|[1.0989836, 0.511...|      abythr1|[abythr1, abythr1...|          142|\n",
      "|  XC227239|        50|       [abythr1]|[combul2, grbcam1...|  [0.84211377519815...|[1.6610856, 0.870...|[1.4390839, 1.616...|[1.5228174, 1.202...|      abythr1|[abythr1, combul2...|          142|\n",
      "|  XC509507|        20|       [abythr1]|           [fotdro5]|  [0.6930028164341019]|[0.9759565, 0.753...|[1.0162606, 1.178...|[0.99444264, 1.14...|      abythr1|  [abythr1, fotdro5]|          142|\n",
      "|  XC620997|        10|       [abythr1]|[abythr1, abythr1...|  [0.96802423963072...|[1.5794071, 0.886...|[1.0814278, 0.572...|[1.3417553, 0.759...|      abythr1|[abythr1, abythr1...|          142|\n",
      "|  XC125862|        50|       [abythr1]|           [abythr1]|  [0.8696589410631123]|[0.9370399, 0.233...|[0.9370399, 0.233...|[1.0989836, 0.511...|      abythr1|  [abythr1, abythr1]|          142|\n",
      "+----------+----------+----------------+--------------------+----------------------+--------------------+--------------------+--------------------+-------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from birdclef.utils import get_spark\n",
    "from pyspark.sql import Window, functions as F\n",
    "import os\n",
    "\n",
    "os.environ[\"SPARK_LOCAL_DIRS\"] = \"../data/tmp/spark\"\n",
    "\n",
    "spark = get_spark(memory=\"5g\")\n",
    "df = spark.read.parquet(\"../data/processed/birdclef-2023/train_postprocessed/v3\")\n",
    "df = (\n",
    "    df.withColumn(\"primary_label\", F.col(\"metadata_species\")[0])\n",
    "    .withColumn(\"species\", F.concat(\"metadata_species\", \"predicted_species\"))\n",
    "    .withColumn(\"species_count\", F.count(\"*\").over(Window.partitionBy(\"primary_label\")))\n",
    ")\n",
    "# df = df.where(\"species_count > 1\")\n",
    "df.printSchema()\n",
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "def model_eval(truth, preds):\n",
    "    print(\"Accuracy:\", accuracy_score(truth, preds))\n",
    "    print(\n",
    "        \"Precision:\",\n",
    "        precision_score(truth, preds, average=\"macro\"),\n",
    "    )\n",
    "    print(\n",
    "        \"Recall:\",\n",
    "        recall_score(truth, preds, average=\"macro\"),\n",
    "    )\n",
    "    print(\n",
    "        \"F1 Score:\",\n",
    "        f1_score(truth, preds, average=\"macro\"),\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "current and next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+\n",
      "|   predicted_species|primary_label|           embedding|      next_embedding|\n",
      "+--------------------+-------------+--------------------+--------------------+\n",
      "|  [abythr1, wbrcha2]|      abythr1|[1.4855427, 0.454...|[0.9370399, 0.233...|\n",
      "|[combul2, grbcam1...|      abythr1|[1.6610856, 0.870...|[1.4390839, 1.616...|\n",
      "|           [fotdro5]|      abythr1|[0.9759565, 0.753...|[1.0162606, 1.178...|\n",
      "|[abythr1, abythr1...|      abythr1|[1.5794071, 0.886...|[1.0814278, 0.572...|\n",
      "|           [abythr1]|      abythr1|[0.9370399, 0.233...|[0.9370399, 0.233...|\n",
      "+--------------------+-------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed = df.select(\"predicted_species\", \"primary_label\", \"embedding\", \"next_embedding\")\n",
    "parsed.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_species</th>\n",
       "      <th>primary_label</th>\n",
       "      <th>embedding</th>\n",
       "      <th>next_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[abythr1, wbrcha2]</td>\n",
       "      <td>abythr1</td>\n",
       "      <td>[1.4855427, 0.4541664, 1.0472355, 1.3523579, 1...</td>\n",
       "      <td>[0.9370399, 0.2331794, 0.5724607, 1.3511919, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[combul2, grbcam1, combuz1]</td>\n",
       "      <td>abythr1</td>\n",
       "      <td>[1.6610856, 0.87099504, 0.29712436, 0.7613897,...</td>\n",
       "      <td>[1.4390839, 1.6169457, 1.8661345, 0.8268714, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[fotdro5]</td>\n",
       "      <td>abythr1</td>\n",
       "      <td>[0.9759565, 0.75392014, 0.6601531, 0.51182395,...</td>\n",
       "      <td>[1.0162606, 1.1786319, 0.6472011, 0.5133413, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[abythr1, abythr1, thrnig1, thrnig1]</td>\n",
       "      <td>abythr1</td>\n",
       "      <td>[1.5794071, 0.8864207, 1.2426404, 1.2989092, 0...</td>\n",
       "      <td>[1.0814278, 0.5720312, 1.1402651, 1.4965066, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[abythr1]</td>\n",
       "      <td>abythr1</td>\n",
       "      <td>[0.9370399, 0.2331794, 0.5724607, 1.3511919, 0...</td>\n",
       "      <td>[0.9370399, 0.2331794, 0.5724607, 1.3511919, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      predicted_species primary_label  \\\n",
       "0                    [abythr1, wbrcha2]       abythr1   \n",
       "1           [combul2, grbcam1, combuz1]       abythr1   \n",
       "2                             [fotdro5]       abythr1   \n",
       "3  [abythr1, abythr1, thrnig1, thrnig1]       abythr1   \n",
       "4                             [abythr1]       abythr1   \n",
       "\n",
       "                                           embedding  \\\n",
       "0  [1.4855427, 0.4541664, 1.0472355, 1.3523579, 1...   \n",
       "1  [1.6610856, 0.87099504, 0.29712436, 0.7613897,...   \n",
       "2  [0.9759565, 0.75392014, 0.6601531, 0.51182395,...   \n",
       "3  [1.5794071, 0.8864207, 1.2426404, 1.2989092, 0...   \n",
       "4  [0.9370399, 0.2331794, 0.5724607, 1.3511919, 0...   \n",
       "\n",
       "                                      next_embedding  \n",
       "0  [0.9370399, 0.2331794, 0.5724607, 1.3511919, 0...  \n",
       "1  [1.4390839, 1.6169457, 1.8661345, 0.8268714, 1...  \n",
       "2  [1.0162606, 1.1786319, 0.6472011, 0.5133413, 0...  \n",
       "3  [1.0814278, 0.5720312, 1.1402651, 1.4965066, 0...  \n",
       "4  [0.9370399, 0.2331794, 0.5724607, 1.3511919, 0...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = parsed.toPandas()\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "# labels = mlb.fit_transform(data[\"predicted_species\"])\n",
    "# print(labels.shape)\n",
    "\n",
    "# embeddings = np.stack(data[\"embedding\"])\n",
    "# next_emb = np.stack(data[\"next_embedding\"])\n",
    "# print(embeddings.shape, next_emb.shape)\n",
    "# emb = np.concatenate((embeddings, next_emb), axis=1)\n",
    "# print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_y(data):\n",
    "    x = np.concatenate(\n",
    "        (np.stack(data.embedding), np.stack(data.next_embedding)), axis=1\n",
    "    )\n",
    "    y = mlb.transform(data.predicted_species)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 predicted_species primary_label  \\\n",
      "46627           [trobou1, trobou1]       gobbun1   \n",
      "9298      [wlwwar, wlwwar, wlwwar]        wlwwar   \n",
      "16062     [gargan, gargan, woosan]        gargan   \n",
      "54580  [eaywag1, eaywag1, combuz1]       eaywag1   \n",
      "31676           [blakit1, blakit1]       blakit1   \n",
      "\n",
      "                                               embedding  \\\n",
      "46627  [0.99324703, 1.2544072, 1.1864733, 0.76876074,...   \n",
      "9298   [1.3639158, 1.1384009, 0.30133072, 1.1934186, ...   \n",
      "16062  [0.9726482, 0.80500895, 0.6744415, 0.58976775,...   \n",
      "54580  [0.37639582, 0.8841811, 0.8896645, 0.40079167,...   \n",
      "31676  [0.7950919, 0.6024793, 0.11996893, 0.8943327, ...   \n",
      "\n",
      "                                          next_embedding  \n",
      "46627  [1.1276547, 1.0443239, 1.4978375, 0.6715452, 0...  \n",
      "9298   [1.2790996, 0.8213889, 0.13639079, 0.62267214,...  \n",
      "16062  [0.7132024, 0.9949344, 0.6074874, 0.43088412, ...  \n",
      "54580  [0.918205, 0.7151461, 0.29273865, 0.5959397, 0...  \n",
      "31676  [0.9469747, 0.51665765, 0.31957522, 1.0933274,...  \n",
      "(57224,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, stratify=data.primary_label)\n",
    "\n",
    "print(train.head(5))\n",
    "\n",
    "weights = compute_sample_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    y=train.primary_label,\n",
    ")\n",
    "print(weights.shape)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(train.predicted_species)\n",
    "train_x, train_y = split_x_y(train)\n",
    "test_x, test_y = split_x_y(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2870823430728366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6341810776088794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.20198595718608378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.2917415148921085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48481025107652603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(tree_method=\"gpu_hist\")\n",
    "clf.fit(train_x, train_y, verbose=True)\n",
    "preds = clf.predict(test_x)\n",
    "model_eval(test_y, preds)\n",
    "print(average_precision_score(test_y, clf.predict_proba(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = BayesSearchCV(\n",
    "    XGBClassifier(tree_method=\"gpu_hist\"),\n",
    "    {\n",
    "        \"max_depth\": (3, 15, \"uniform\"),\n",
    "        \"gamma\": (0.0, 1.0, \"uniform\"),\n",
    "        \"min_child_weight\": (1, 10, \"uniform\"),\n",
    "    },\n",
    "    fit_params={\"sample_weight\": weights},\n",
    "    n_iter=8,\n",
    "    scoring=\"f1_macro\",\n",
    "    verbose=4,\n",
    "    cv=2,\n",
    "    n_points=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "search.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+--------------------+\n",
      "|   predicted_species|primary_label|           embedding|      next_embedding|     track_embedding|\n",
      "+--------------------+-------------+--------------------+--------------------+--------------------+\n",
      "|[categr, cohmar1,...|       categr|[1.6699009, 1.923...|[1.6699009, 1.923...|[1.6699009, 1.923...|\n",
      "|    [egygoo, egygoo]|       egygoo|[1.1547908, 1.127...|[1.4386853, 1.303...|[1.5258964, 1.228...|\n",
      "|[bawhor2, bawhor2...|      bawhor2|[0.4971428, 2.136...|[0.99904233, 1.15...|[0.768064, 1.3247...|\n",
      "|  [cibwar1, cibwar1]|      cibwar1|[0.75827354, 1.25...|[0.80793333, 1.24...|[0.78310347, 1.25...|\n",
      "|           [eubeat1]|      eubeat1|[1.7087168, 1.229...|[2.0123289, 0.626...|[1.7032318, 1.002...|\n",
      "+--------------------+-------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_species</th>\n",
       "      <th>primary_label</th>\n",
       "      <th>embedding</th>\n",
       "      <th>next_embedding</th>\n",
       "      <th>track_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[categr, cohmar1, sichor1]</td>\n",
       "      <td>categr</td>\n",
       "      <td>[1.6699009, 1.9233328, 0.37807903, 1.2282478, ...</td>\n",
       "      <td>[1.6699009, 1.9233328, 0.37807903, 1.2282478, ...</td>\n",
       "      <td>[1.6699009, 1.9233328, 0.37807903, 1.2282478, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[egygoo, egygoo]</td>\n",
       "      <td>egygoo</td>\n",
       "      <td>[1.1547908, 1.1271043, 0.36353567, 0.8874313, ...</td>\n",
       "      <td>[1.4386853, 1.303846, 0.23873292, 1.1468445, 1...</td>\n",
       "      <td>[1.5258964, 1.2280338, 0.4696629, 1.0480278, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bawhor2, bawhor2, bawhor2, bawhor2, grecor]</td>\n",
       "      <td>bawhor2</td>\n",
       "      <td>[0.4971428, 2.1368887, 0.69026494, 1.0474387, ...</td>\n",
       "      <td>[0.99904233, 1.1514673, 0.62358767, 1.1789998,...</td>\n",
       "      <td>[0.768064, 1.324733, 0.79627794, 1.0657245, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[cibwar1, cibwar1]</td>\n",
       "      <td>cibwar1</td>\n",
       "      <td>[0.75827354, 1.2599143, 1.2712938, 1.2929367, ...</td>\n",
       "      <td>[0.80793333, 1.2478226, 1.50593, 1.0412738, 0....</td>\n",
       "      <td>[0.78310347, 1.2538685, 1.3886118, 1.1671052, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[eubeat1]</td>\n",
       "      <td>eubeat1</td>\n",
       "      <td>[1.7087168, 1.2295375, 0.5481695, 0.48683575, ...</td>\n",
       "      <td>[2.0123289, 0.6269798, 0.7979782, 0.42369375, ...</td>\n",
       "      <td>[1.7032318, 1.0029125, 0.46927804, 0.48526886,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              predicted_species primary_label  \\\n",
       "0                    [categr, cohmar1, sichor1]        categr   \n",
       "1                              [egygoo, egygoo]        egygoo   \n",
       "2  [bawhor2, bawhor2, bawhor2, bawhor2, grecor]       bawhor2   \n",
       "3                            [cibwar1, cibwar1]       cibwar1   \n",
       "4                                     [eubeat1]       eubeat1   \n",
       "\n",
       "                                           embedding  \\\n",
       "0  [1.6699009, 1.9233328, 0.37807903, 1.2282478, ...   \n",
       "1  [1.1547908, 1.1271043, 0.36353567, 0.8874313, ...   \n",
       "2  [0.4971428, 2.1368887, 0.69026494, 1.0474387, ...   \n",
       "3  [0.75827354, 1.2599143, 1.2712938, 1.2929367, ...   \n",
       "4  [1.7087168, 1.2295375, 0.5481695, 0.48683575, ...   \n",
       "\n",
       "                                      next_embedding  \\\n",
       "0  [1.6699009, 1.9233328, 0.37807903, 1.2282478, ...   \n",
       "1  [1.4386853, 1.303846, 0.23873292, 1.1468445, 1...   \n",
       "2  [0.99904233, 1.1514673, 0.62358767, 1.1789998,...   \n",
       "3  [0.80793333, 1.2478226, 1.50593, 1.0412738, 0....   \n",
       "4  [2.0123289, 0.6269798, 0.7979782, 0.42369375, ...   \n",
       "\n",
       "                                     track_embedding  \n",
       "0  [1.6699009, 1.9233328, 0.37807903, 1.2282478, ...  \n",
       "1  [1.5258964, 1.2280338, 0.4696629, 1.0480278, 1...  \n",
       "2  [0.768064, 1.324733, 0.79627794, 1.0657245, 0....  \n",
       "3  [0.78310347, 1.2538685, 1.3886118, 1.1671052, ...  \n",
       "4  [1.7032318, 1.0029125, 0.46927804, 0.48526886,...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed = df.select(\n",
    "    \"predicted_species\",\n",
    "    \"primary_label\",\n",
    "    \"embedding\",\n",
    "    \"next_embedding\",\n",
    "    \"track_embedding\",\n",
    ")\n",
    "parsed.show(n=5)\n",
    "data = parsed.toPandas()\n",
    "data.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               predicted_species primary_label  \\\n",
      "73358          [yebsto1, barswa]       yebsto1   \n",
      "42649                  [cohmar1]        litegr   \n",
      "38661  [litegr, barswa, yertin1]        litegr   \n",
      "41084           [gargan, gargan]        gargan   \n",
      "31211           [wlwwar, hoopoe]       tafpri1   \n",
      "\n",
      "                                               embedding  \\\n",
      "73358  [0.80422944, 0.68525517, 0.17662634, 0.493083,...   \n",
      "42649  [0.8279805, 0.92492956, 1.0016271, 0.44183776,...   \n",
      "38661  [1.3842217, 1.0944815, 1.3357898, 0.9954681, 1...   \n",
      "41084  [1.2485094, 1.2730036, 0.4728993, 1.344677, 0....   \n",
      "31211  [1.9700922, 1.7753143, 0.3254336, 0.455853, 1....   \n",
      "\n",
      "                                          next_embedding  \\\n",
      "73358  [0.80422944, 0.68525517, 0.17662634, 0.493083,...   \n",
      "42649  [0.66272324, 0.82441515, 0.62881213, 0.3760345...   \n",
      "38661  [1.4363765, 1.5221066, 1.1466409, 0.44075704, ...   \n",
      "41084  [1.2485094, 1.2730036, 0.4728993, 1.344677, 0....   \n",
      "31211  [1.9700922, 1.7753143, 0.3254336, 0.455853, 1....   \n",
      "\n",
      "                                         track_embedding  \n",
      "73358  [0.70979995, 0.6547366, 0.19224243, 0.4996634,...  \n",
      "42649  [0.7696675, 0.8249083, 0.67350805, 0.5226109, ...  \n",
      "38661  [1.2667592, 1.412679, 1.2196978, 0.84407115, 1...  \n",
      "41084  [1.2485094, 1.2730036, 0.4728993, 1.344677, 0....  \n",
      "31211  [1.5685477, 1.5331529, 0.33459786, 0.429743, 1...  \n",
      "(61450,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "def split_x_y(data):\n",
    "    x = np.concatenate(\n",
    "        (\n",
    "            np.stack(data.embedding),\n",
    "            np.stack(data.next_embedding),\n",
    "            np.stack(data.track_embedding),\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    y = mlb.transform(data.predicted_species)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, stratify=data.primary_label)\n",
    "\n",
    "print(train.head(5))\n",
    "\n",
    "weights = compute_sample_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    y=train.primary_label,\n",
    ")\n",
    "print(weights.shape)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(data.predicted_species)\n",
    "train_x, train_y = split_x_y(train)\n",
    "test_x, test_y = split_x_y(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3048371312735915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6907287964057018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.3036790183529477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.40340066136931235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5434118400483244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nzhon\\Codes\\birdclef-2023\\.venv\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:891: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "clf = XGBClassifier(tree_method=\"gpu_hist\")\n",
    "clf.fit(train_x, train_y, verbose=True)\n",
    "preds = clf.predict(test_x)\n",
    "model_eval(test_y, preds)\n",
    "print(average_precision_score(test_y, clf.predict_proba(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    }
   ],
   "source": [
    "search = BayesSearchCV(\n",
    "    XGBClassifier(tree_method=\"gpu_hist\", objective=\"multi:softprob\"),\n",
    "    {\n",
    "        \"max_depth\": (3, 15, \"uniform\"),\n",
    "        \"gamma\": (0.0, 1.0, \"uniform\"),\n",
    "        \"min_child_weight\": (1, 10, \"uniform\"),\n",
    "    },\n",
    "    fit_params={\"sample_weight\": weights},\n",
    "    n_iter=8,\n",
    "    scoring=\"f1_macro\",\n",
    "    verbose=4,\n",
    "    cv=2,\n",
    "    n_points=1,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "search.fit(train_x, train_y)\n",
    "model_eval(test_y, search.predict(test_x))\n",
    "print(average_precision_score(test_y, search.predict_proba(test_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
