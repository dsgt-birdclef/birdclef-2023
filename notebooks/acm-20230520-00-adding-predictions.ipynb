{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding predictions to the dataset using model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from birdclef.utils import get_spark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# modify cores and memory as needed\n",
    "spark = get_spark(cores=24, memory=\"30g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- species: string (nullable = true)\n",
      " |-- track_stem: string (nullable = true)\n",
      " |-- track_type: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- embedding: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- prediction_vec: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- predictions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- rank: long (nullable = true)\n",
      " |    |    |-- index: long (nullable = true)\n",
      " |    |    |-- label: string (nullable = true)\n",
      " |    |    |-- mapped_label: string (nullable = true)\n",
      " |    |    |-- probability: double (nullable = true)\n",
      " |-- start_time: long (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3418610"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/processed/birdclef-2023/train_embeddings/consolidated_v4/\"\n",
    "df = spark.read.parquet(path)\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dataset with baseline model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Load model from pickle file\n",
    "model_path = Path(\"../data/models/baseline/logistic_negative_new.pkl\")\n",
    "clf = pickle.loads(model_path.read_bytes())\n",
    "print(clf.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\code\\kaggle\\birdclef-2023\\venv\\lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "\n",
    "# Predictions\n",
    "@pandas_udf(\"string\", PandasUDFType.SCALAR)\n",
    "def make_prediction(embedding_series: pd.Series) -> pd.Series:\n",
    "    # Convert series of lists to 2D array\n",
    "    embedding_array = np.vstack(embedding_series.tolist())\n",
    "    # Make predictions\n",
    "    predictions = clf.predict(embedding_array)\n",
    "    return pd.Series(predictions)\n",
    "\n",
    "\n",
    "# Probabilities\n",
    "@pandas_udf(\"double\", PandasUDFType.SCALAR)\n",
    "def make_prediction_proba(embedding_series: pd.Series) -> pd.Series:\n",
    "    # Convert series of lists to 2D array\n",
    "    embedding_array = np.vstack(embedding_series.tolist())\n",
    "    # Get prediction probabilities\n",
    "    probabilities = clf.predict_proba(embedding_array)\n",
    "    max_probabilities = np.amax(probabilities, axis=1)\n",
    "    return pd.Series(max_probabilities)\n",
    "\n",
    "\n",
    "# Use the UDF to add predictions and probabilities to your dataframe\n",
    "preds_df = df.withColumn(\"prediction\", make_prediction(df[\"embedding\"]))\n",
    "proba_df = preds_df.withColumn(\"probability\", make_prediction_proba(df[\"embedding\"]))\n",
    "res = proba_df.select(\n",
    "    \"track_name\",\n",
    "    \"start_time\",\n",
    "    \"prediction\",\n",
    "    \"probability\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+\n",
      "|          track_name|start_time|prediction|        probability|\n",
      "+--------------------+----------+----------+-------------------+\n",
      "|grecor/XC629875_p...|        75|   combuz1|  0.595202838244718|\n",
      "|grecor/XC629875_p...|       127|    strher|  0.519304999398772|\n",
      "|grecor/XC629875_p...|       143|    hoopoe|0.39469672179585924|\n",
      "|grecor/XC629875_p...|       100|    barswa|  0.522068624744396|\n",
      "|grecor/XC629875_p...|        62|    egygoo| 0.8028985925544311|\n",
      "|grecor/XC629875_p...|        10|   no_call| 0.8533084422827208|\n",
      "|grecor/XC629875_p...|       139|    wlwwar| 0.5278238795398049|\n",
      "|grecor/XC629875_p...|        25|   combuz1|0.40135901403767077|\n",
      "|grecor/XC629875_p...|        83|    barswa| 0.1204648163949473|\n",
      "|grecor/XC629875_p...|        53|   yertin1| 0.5402404945683114|\n",
      "|grecor/XC629875_p...|         1|   thrnig1|0.39749713513198004|\n",
      "|grecor/XC629875_p...|        34|   gobbun1| 0.7619074297015382|\n",
      "|grecor/XC629875_p...|       123|   cibwar1|0.29400818971870024|\n",
      "|grecor/XC629875_p...|        75|    egygoo|0.49618176979327583|\n",
      "|grecor/XC629875_p...|        88|   no_call| 0.6119368681533695|\n",
      "|grecor/XC629875_p...|         4|   no_call| 0.7579477074191442|\n",
      "|grecor/XC629875_p...|        24|   combuz1| 0.4292204932133836|\n",
      "|grecor/XC629875_p...|         0|    barswa| 0.6783107507637689|\n",
      "|grecor/XC629875_p...|       113|   ratcis1|0.18859484937201154|\n",
      "|grecor/XC629875_p...|        97|   combuz1| 0.5613558960703205|\n",
      "+--------------------+----------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out res to a parquet file, using 1 or 2 partitions.\n",
    "# Use the processed/birdnet-2023 folder and make a new dataset under there.\n",
    "res.repartition(1).write.mode(\"overwrite\").parquet(\n",
    "    \"../data/processed/birdclef-2023/consolidated_v4_with_preds\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
