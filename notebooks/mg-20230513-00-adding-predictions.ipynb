{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup\n",
    "\n",
    "### downloading the dataset\n",
    "\n",
    "Make sure that you have the dataset downloaded locally.\n",
    "At the root of the project, run this command:\n",
    "\n",
    "```bash\n",
    "gsutil -m rsync \\\n",
    "    gs://birdclef-2023/data/processed/birdclef-2023/train_embeddings/consolidated_v3_pre1/ \\\n",
    "    data/processed/birdclef-2023/train_embeddings/consolidated_v3_pre1/ \n",
    "```\n",
    "\n",
    "### using spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/15 03:30:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from birdclef.utils import get_spark\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# modify cores and memory as needed\n",
    "spark = get_spark(cores=8, memory=\"16g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- species: string (nullable = true)\n",
      " |-- track_stem: string (nullable = true)\n",
      " |-- track_type: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- embedding: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- prediction_vec: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- predictions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- rank: long (nullable = true)\n",
      " |    |    |-- index: long (nullable = true)\n",
      " |    |    |-- label: string (nullable = true)\n",
      " |    |    |-- mapped_label: string (nullable = true)\n",
      " |    |    |-- probability: double (nullable = true)\n",
      " |-- start_time: long (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1198860"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/processed/birdclef-2023/train_embeddings/consolidated_v3/\"\n",
    "df = spark.read.parquet(path)\n",
    "df.printSchema()\n",
    "df.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dataset with baseline model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Load model from pickle file\n",
    "model_path = Path(\"../data/models/baseline/logistic_negative_new.pkl\")\n",
    "clf = pickle.loads(model_path.read_bytes())\n",
    "print(clf.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "\n",
    "# Predictions\n",
    "@pandas_udf(\"string\", PandasUDFType.SCALAR)\n",
    "def make_prediction(embedding_series: pd.Series) -> pd.Series:\n",
    "    # Convert series of lists to 2D array\n",
    "    embedding_array = np.vstack(embedding_series.tolist())\n",
    "    # Make predictions\n",
    "    predictions = clf.predict(embedding_array)\n",
    "    return pd.Series(predictions)\n",
    "\n",
    "\n",
    "# Probabilities\n",
    "@pandas_udf(\"double\", PandasUDFType.SCALAR)\n",
    "def make_prediction_proba(embedding_series: pd.Series) -> pd.Series:\n",
    "    # Convert series of lists to 2D array\n",
    "    embedding_array = np.vstack(embedding_series.tolist())\n",
    "    # Get prediction probabilities\n",
    "    probabilities = clf.predict_proba(embedding_array)\n",
    "    max_probabilities = np.amax(probabilities, axis=1)\n",
    "    return pd.Series(max_probabilities)\n",
    "\n",
    "\n",
    "# Use the UDF to add predictions and probabilities to your dataframe\n",
    "preds_df = df.withColumn(\"prediction\", make_prediction(df[\"embedding\"]))\n",
    "proba_df = preds_df.withColumn(\"probability\", make_prediction_proba(df[\"embedding\"]))\n",
    "res = proba_df.select(\n",
    "    \"track_name\",\n",
    "    \"start_time\",\n",
    "    \"prediction\",\n",
    "    \"probability\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/15 03:30:29 WARN DAGScheduler: Broadcasting large task binary with size 1372.8 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+-------------------+\n",
      "|          track_name|start_time|prediction|        probability|\n",
      "+--------------------+----------+----------+-------------------+\n",
      "|wlwwar/XC475384_p...|        18|    wlwwar| 0.5675699565354009|\n",
      "|grecor/XC629875_p...|        60|   no_call|0.48938898129941916|\n",
      "|grecor/XC629875_p...|        87|    litegr| 0.9345325524879964|\n",
      "|wlwwar/XC475384_p...|        51|    wlwwar| 0.9943723441150192|\n",
      "|grecor/XC629875_p...|        15|    strher| 0.4401947783710894|\n",
      "|grecor/XC629875_p...|        96|   combuz1| 0.4643792294901046|\n",
      "|grecor/XC629875_p...|        60|   combuz1|0.40203665348791984|\n",
      "|wlwwar/XC475384_p...|        66|    wlwwar| 0.5959878192798792|\n",
      "|wlwwar/XC475384_p...|        21|   combuz1|0.47864665964721836|\n",
      "|grecor/XC629875_p...|       117|   combuz1|0.47142785258506426|\n",
      "|wlwwar/XC475384_p...|         9|   thrnig1|0.37728004342577076|\n",
      "|grecor/XC629875_p...|        21|    egygoo| 0.7234628309654835|\n",
      "|grecor/XC629875_p...|        84|   thrnig1| 0.5529269869278212|\n",
      "|grecor/XC629875_p...|        51|    greegr| 0.8697011363387107|\n",
      "|grecor/XC629875_p...|        78|   thrnig1| 0.5633442867283879|\n",
      "|wlwwar/XC475384_p...|       108|    wlwwar| 0.9933693780001539|\n",
      "|wlwwar/XC475384_p...|        48|   combuz1| 0.3125566246021811|\n",
      "|grecor/XC629875_p...|        93|   no_call|  0.845770746866694|\n",
      "|grecor/XC629875_p...|       147|    hoopoe| 0.5377745115749114|\n",
      "|wlwwar/XC475384_p...|        27|   combuz1|0.49392749556259213|\n",
      "+--------------------+----------+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/15 03:30:40 WARN DAGScheduler: Broadcasting large task binary with size 1374.5 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write out res to a parquet file, using 1 or 2 partitions.\n",
    "# Use the processed/birdnet-2023 folder and make a new dataset under there.\n",
    "res.repartition(2).write.mode(\"overwrite\").parquet(\n",
    "    \"../data/processed/birdnet-2023/consolidated_v3_with_preds\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
