{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier (call/no call) using the Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/06 14:50:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- track_stem: string (nullable = true)\n",
      " |-- track_type: string (nullable = true)\n",
      " |-- start_time: long (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      " |-- embedding: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- prediction_vec: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from birdclef.utils import get_spark\n",
    "\n",
    "spark = get_spark(cores=4, memory=\"10g\")\n",
    "# Load train_postprocessed/v7 dataset\n",
    "df = spark.read.parquet(\"../data/processed/birdclef-2023/train_postprocessed/v7\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_id: string (nullable = true)\n",
      " |-- emb: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- call: double (nullable = true)\n",
      " |-- no_call: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import background noise dataset\n",
    "back_noise_df = spark.read.parquet(\"../data/processed/birdclef-2023/background_noise\")\n",
    "back_noise_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8136"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_noise_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- row_id: string (nullable = true)\n",
      " |-- emb: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- call: double (nullable = true)\n",
      " |-- no_call: double (nullable = true)\n",
      "\n",
      "+-------+--------------------+\n",
      "|species|           embedding|\n",
      "+-------+--------------------+\n",
      "|no_call|[0.92088675498962...|\n",
      "|no_call|[0.84365564584732...|\n",
      "|no_call|[1.26694309711456...|\n",
      "|no_call|[1.38729619979858...|\n",
      "|no_call|[0.67062968015670...|\n",
      "|no_call|[0.55484676361083...|\n",
      "|no_call|[0.76140671968460...|\n",
      "|no_call|[1.05057728290557...|\n",
      "|no_call|[1.10778868198394...|\n",
      "|no_call|[0.97304064035415...|\n",
      "|no_call|[0.80366438627243...|\n",
      "|no_call|[0.83019882440567...|\n",
      "|no_call|[1.27283263206481...|\n",
      "|no_call|[1.18015515804290...|\n",
      "|no_call|[1.46877062320709...|\n",
      "|no_call|[1.15537261962890...|\n",
      "|no_call|[1.25248849391937...|\n",
      "|no_call|[1.28011035919189...|\n",
      "|no_call|[1.57398808002471...|\n",
      "|no_call|[1.14994704723358...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Import background noise dataset\n",
    "back_noise_df = spark.read.parquet(\"../data/processed/birdclef-2023/background_noise\")\n",
    "back_noise_df.printSchema()\n",
    "\n",
    "# Add a 'species' column to back_noise_df with 'no_call' as values\n",
    "back_noise_df = back_noise_df.withColumn(\"species\", lit(\"no_call\"))\n",
    "back_noise_df = back_noise_df.withColumnRenamed(\"emb\", \"embedding\")\n",
    "\n",
    "# Make sure 'species' is the first column to math the structure of negatives DF\n",
    "back_noise_sub = back_noise_df.select(\"species\", \"embedding\")\n",
    "# negatives_sub = negatives.select(\"species\", \"embedding\")\n",
    "\n",
    "# Union the two DFs\n",
    "# negatives_df = negatives_sub.union(back_noise_sub)\n",
    "\n",
    "# Select positive samples and Union with negatives\n",
    "positives_sub = df.select(\"species\", \"embedding\")\n",
    "positives_df = positives_sub.withColumn(\"species\", lit(\"call\"))\n",
    "binary_df = back_noise_sub.union(positives_df)\n",
    "binary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|species|     n|\n",
      "+-------+------+\n",
      "|   call|255372|\n",
      "|no_call|  8136|\n",
      "+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window, functions as F\n",
    "\n",
    "# Check the number of samples for each label\n",
    "counts = binary_df.groupBy(\"species\").agg(F.count(\"*\").alias(\"n\")).orderBy(F.desc(\"n\"))\n",
    "counts.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
