{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext lab_black"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Classifier\n",
    "\n",
    "Using the [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) from sklearn to train the `consolidaded_v3` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/20 18:26:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/20 18:26:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- species: string (nullable = true)\n",
      " |-- track_stem: string (nullable = true)\n",
      " |-- track_type: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- embedding: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- prediction_vec: array (nullable = true)\n",
      " |    |-- element: double (containsNull = true)\n",
      " |-- predictions: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- rank: long (nullable = true)\n",
      " |    |    |-- index: long (nullable = true)\n",
      " |    |    |-- label: string (nullable = true)\n",
      " |    |    |-- mapped_label: string (nullable = true)\n",
      " |    |    |-- probability: double (nullable = true)\n",
      " |-- start_time: long (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from birdclef.utils import get_spark\n",
    "\n",
    "spark = get_spark(cores=16, memory=\"20g\")\n",
    "df = spark.read.parquet(\n",
    "    \"../data/processed/birdclef-2023/train_embeddings/consolidated_v3\"\n",
    "    # \"../data/processed/birdclef-2023/train_embeddings/consolidated_v4\"\n",
    ")\n",
    "df.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===================================================>   (187 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+--------------------+----+-----+--------------------+------------+--------------------+\n",
      "|species|track_stem|track_type|start_time|          track_name|rank|index|               label|mapped_label|         probability|\n",
      "+-------+----------+----------+----------+--------------------+----+-----+--------------------+------------+--------------------+\n",
      "|abythr1|  XC233199|   source0|         0|abythr1/XC233199_...|   0|  639|Chloropsis hardwi...|     orblea1|0.002208352088928...|\n",
      "|abythr1|  XC233199|   source0|        57|abythr1/XC233199_...|   0| 1151|Erpornis zanthole...|     whbyuh1|0.025502817705273628|\n",
      "|abythr1|  XC233199|   source0|        27|abythr1/XC233199_...|   0| 3164|Turdus abyssinicu...|     abythr1|0.024902962148189545|\n",
      "|abythr1|  XC233199|   source0|        30|abythr1/XC233199_...|   0|  639|Chloropsis hardwi...|     orblea1|0.012038093991577625|\n",
      "|abythr1|  XC233199|   source0|        21|abythr1/XC233199_...|   0| 3185|Turdus leucomelas...|     pabthr1|  0.7510073781013489|\n",
      "+-------+----------+----------+----------+--------------------+----+-----+--------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window, functions as F\n",
    "\n",
    "# keep the track_type for the highest energy\n",
    "highest_energy_channel = (\n",
    "    df\n",
    "    # get the track stem without the part\n",
    "    .withColumn(\"original_track_stem\", F.split(F.col(\"track_stem\"), \"_\").getItem(0))\n",
    "    .where(\"track_type != 'original'\")\n",
    "    # get the track type that has the most energy\n",
    "    .withColumn(\n",
    "        \"rank\",\n",
    "        F.rank().over(\n",
    "            Window.partitionBy(\"original_track_stem\").orderBy(F.desc(\"energy\"))\n",
    "        ),\n",
    "    )\n",
    "    # keep the first row\n",
    "    .where(F.col(\"rank\") == 1)\n",
    "    # drop the rank column\n",
    "    .select(\"species\", \"track_stem\", \"track_type\")\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "# get the highest predictions by exploding the values\n",
    "exploded_embeddings = (\n",
    "    df\n",
    "    # join against the highest energy channel\n",
    "    .join(\n",
    "        highest_energy_channel,\n",
    "        on=[\"species\", \"track_stem\", \"track_type\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    # explode the embeddings, these are ordered by confidence\n",
    "    .withColumn(\"predictions\", F.explode(\"predictions\")).select(\n",
    "        \"species\",\n",
    "        \"track_stem\",\n",
    "        \"track_type\",\n",
    "        \"start_time\",\n",
    "        \"track_name\",\n",
    "        \"embedding\",\n",
    "        \"predictions.*\",\n",
    "    )\n",
    "    # simplifying assumption: we assume the prediction with the highest confidence is the true label\n",
    "    .where(\"rank = 0\")\n",
    ").cache()\n",
    "\n",
    "exploded_embeddings.drop(\"embedding\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|species|    n|\n",
      "+-------+-----+\n",
      "|thrnig1|12987|\n",
      "| wlwwar| 9249|\n",
      "|combuz1| 7173|\n",
      "| hoopoe| 6731|\n",
      "| barswa| 6191|\n",
      "+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===============================================>      (176 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|species|  n|\n",
      "+-------+---+\n",
      "|afpkin1|  3|\n",
      "|whhsaw1|  4|\n",
      "|whctur2|  4|\n",
      "|golher1|  5|\n",
      "|lotlap1|  8|\n",
      "+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# quick count of the number of samples\n",
    "counts = (\n",
    "    exploded_embeddings.groupBy(\"species\")\n",
    "    .agg(F.count(\"*\").alias(\"n\"))\n",
    "    .orderBy(F.desc(\"n\"))\n",
    ")\n",
    "counts.show(n=5)\n",
    "counts.orderBy(\"n\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|species|  n|\n",
      "+-------+---+\n",
      "|purgre2| 60|\n",
      "|bubwar2| 90|\n",
      "|rehwea1| 69|\n",
      "|kvbsun1| 80|\n",
      "|equaka1| 63|\n",
      "+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+\n",
      "|species|       probability|           embedding|\n",
      "+-------+------------------+--------------------+\n",
      "|afghor1|0.9965255856513977|[0.57833033800125...|\n",
      "|afghor1| 0.511886715888977|[1.00166213512420...|\n",
      "|afghor1|0.9984956979751587|[0.88829582929611...|\n",
      "|afghor1|0.9988522529602051|[1.26016914844512...|\n",
      "|afghor1|0.9997662901878357|[1.16302716732025...|\n",
      "+-------+------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "74490"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rarity_min_count = 100\n",
    "rare_species_count = (\n",
    "    exploded_embeddings.groupBy(\"species\")\n",
    "    .agg(F.count(\"*\").alias(\"n\"))\n",
    "    .where(f\"n < {rarity_min_count}\")\n",
    ")\n",
    "rare_species_count.show(n=5)\n",
    "\n",
    "# if there are a lot of examples, we can use a higher threshold\n",
    "common_species = exploded_embeddings.where(\"probability > 0.4\").join(\n",
    "    rare_species_count.select(\"species\"), on=\"species\", how=\"left_anti\"\n",
    ")\n",
    "# these ones are less common so we use a lower threshold so we have at least one\n",
    "# example for each species\n",
    "rare_species = exploded_embeddings.where(\"probability > 0.05\").join(\n",
    "    rare_species_count.select(\"species\"), on=\"species\", how=\"inner\"\n",
    ")\n",
    "prepared = common_species.union(rare_species).select(\n",
    "    \"species\", \"probability\", \"embedding\"\n",
    ")\n",
    "prepared.show(n=5)\n",
    "prepared.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of species 264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|species|   n|\n",
      "+-------+----+\n",
      "|thrnig1|3833|\n",
      "| hoopoe|3822|\n",
      "|eubeat1|3116|\n",
      "| wlwwar|2687|\n",
      "| barswa|2603|\n",
      "+-------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|species|  n|\n",
      "+-------+---+\n",
      "|afpkin1|  2|\n",
      "|whctur2|  2|\n",
      "|rehblu1|  2|\n",
      "|whhsaw1|  3|\n",
      "|easmog1|  4|\n",
      "+-------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets check that we have the right number of classes, and how many examples we are working with\n",
    "prepared_counts = (\n",
    "    prepared.groupBy(\"species\").agg(F.count(\"*\").alias(\"n\")).orderBy(F.desc(\"n\"))\n",
    ")\n",
    "print(f\"number of species {prepared_counts.count()}\")\n",
    "\n",
    "prepared_counts.show(n=5)\n",
    "prepared_counts.orderBy(\"n\").show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>probability</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afghor1</td>\n",
       "      <td>0.996526</td>\n",
       "      <td>[0.5783303380012512, 1.845029354095459, 0.2178...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afghor1</td>\n",
       "      <td>0.511887</td>\n",
       "      <td>[1.0016621351242065, 1.2551445960998535, 0.242...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afghor1</td>\n",
       "      <td>0.998496</td>\n",
       "      <td>[0.8882958292961121, 1.4398638010025024, 0.195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afghor1</td>\n",
       "      <td>0.998852</td>\n",
       "      <td>[1.2601691484451294, 2.366661787033081, 0.2103...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afghor1</td>\n",
       "      <td>0.999766</td>\n",
       "      <td>[1.1630271673202515, 1.7402706146240234, 0.020...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species  probability                                          embedding\n",
       "0  afghor1     0.996526  [0.5783303380012512, 1.845029354095459, 0.2178...\n",
       "1  afghor1     0.511887  [1.0016621351242065, 1.2551445960998535, 0.242...\n",
       "2  afghor1     0.998496  [0.8882958292961121, 1.4398638010025024, 0.195...\n",
       "3  afghor1     0.998852  [1.2601691484451294, 2.366661787033081, 0.2103...\n",
       "4  afghor1     0.999766  [1.1630271673202515, 1.7402706146240234, 0.020..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data for model training\n",
    "data = prepared.toPandas()\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MLPClassifier model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49908, 320) (24582, 320)\n",
      "(49908,) (24582,)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    np.stack(data[\"embedding\"]),\n",
    "    data[\"species\"],\n",
    "    test_size=0.33,\n",
    "    stratify=data[\"species\"],\n",
    ")\n",
    "\n",
    "# Data shape\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 320) (1000, 320)\n",
      "(5000,) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Select a small subset of data to train the model and test the class implementation\n",
    "# After training on the data subset, use the original train/test split data to train the model\n",
    "X_train_sub = X_train[:5000]\n",
    "X_test_sub = X_test[:1000]\n",
    "y_train_sub = y_train[:5000]\n",
    "y_test_sub = y_test[:1000]\n",
    "\n",
    "# Data shape\n",
    "print(X_train_sub.shape, X_test_sub.shape)\n",
    "print(y_train_sub.shape, y_test_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner class\n",
    "class Learner:\n",
    "    def __init__(self, pipe, params):\n",
    "        self.pipe = pipe\n",
    "        self.params = params\n",
    "        self.clf = None\n",
    "        self.scores = None\n",
    "        self.search_name = None\n",
    "        self.class_report = None\n",
    "        self.dataset_name = None\n",
    "        self.learning_curve = {}\n",
    "        self.validation_curve = {}\n",
    "        self.cv = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "        self.name = str(self.pipe[\"model\"].__class__.__name__)\n",
    "\n",
    "    def fit_gridsearch(self, search_func, X_train, y_train, verbose=False):\n",
    "        \"\"\"\n",
    "        Method to train the model using a search algorithm.\n",
    "\n",
    "        search_func: GridSearchCV, RandomizedSearchCV from sklearn.\n",
    "        X_train: training features dataset.\n",
    "        y_train: training labels dataset.\n",
    "        verbose: int() Controls the verbosity: the higher, the more messages (1, 2, or 3).\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "\n",
    "        # Train learner\n",
    "        self.clf = search_func(\n",
    "            self.pipe,\n",
    "            self.params,\n",
    "            scoring={\n",
    "                \"accuracy\": make_scorer(accuracy_score),\n",
    "                \"precision\": make_scorer(precision_score),\n",
    "                \"recall\": make_scorer(recall_score),\n",
    "                \"f1\": make_scorer(f1_score),\n",
    "            },\n",
    "            refit=\"f1\",\n",
    "            cv=self.cv,\n",
    "            verbose=verbose,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        # Fit the model\n",
    "        self.clf.fit(X_train, y_train)\n",
    "        self.search_name = str(self.clf.__class__.__name__)\n",
    "\n",
    "    def get_scores(self, X_train, X_test, y_train, y_test, average=None):\n",
    "        \"\"\"\n",
    "        Method to get model scores.\n",
    "\n",
    "        X_train: training features dataset.\n",
    "        X_test: test features dataset.\n",
    "        y_train: training labels dataset.\n",
    "        y_test: test labels dataset.\n",
    "        \"\"\"\n",
    "        if self.search_name == \"Benchmark\":\n",
    "            best_estimator = self.clf\n",
    "        else:\n",
    "            best_estimator = self.clf.best_estimator_\n",
    "\n",
    "        np.random.seed(42)\n",
    "        # Score on training data\n",
    "        start_time = time.time()\n",
    "        best_estimator.fit(X_train, y_train)\n",
    "        end_time = time.time()\n",
    "        wall_clock_fit = end_time - start_time\n",
    "        # train_score = self.clf.score(X_train, y_train)\n",
    "        train_score = best_estimator.score(X_train, y_train)\n",
    "\n",
    "        # Score on test data\n",
    "        start_time = time.time()\n",
    "        # y_pred = self.clf.predict(X_test)\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "        end_time = time.time()\n",
    "        wall_clock_pred = end_time - start_time\n",
    "        # test_score = self.clf.score(X_test, y_test)\n",
    "        test_score = best_estimator.score(X_test, y_test)\n",
    "        # Metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average=average)\n",
    "        recall = recall_score(y_test, y_pred, average=average)\n",
    "        f1 = f1_score(y_test, y_pred, average=average)\n",
    "        # Classification report\n",
    "        self.class_report = classification_report(y_test, y_pred)\n",
    "\n",
    "        self.scores = {\n",
    "            \"train_score\": round(train_score, 3),\n",
    "            \"test_score\": round(test_score, 3),\n",
    "            \"accuracy\": round(accuracy, 3),\n",
    "            \"precision\": round(precision, 3),\n",
    "            \"recall\": round(recall, 3),\n",
    "            \"f1\": round(f1, 3),\n",
    "            \"wall_clock_fit\": wall_clock_fit,\n",
    "            \"wall_clock_pred\": wall_clock_pred,\n",
    "        }\n",
    "\n",
    "    # Evaluate Learner class\n",
    "    def evaluate_learner(self):\n",
    "        \"\"\"\n",
    "        Print model scores\n",
    "        \"\"\"\n",
    "        print(f\"{'#################################'*2}\")\n",
    "        print(f\"{self.search_name}:\\t  {self.name}\")\n",
    "        print(f\"Train score:     {round(self.scores['train_score'], 3)}\")\n",
    "        print(f\"Test score:      {round(self.scores['test_score'], 3)}\")\n",
    "        print(f\"Accuracy score:  {round(self.scores['accuracy'], 3)}\")\n",
    "        print(f\"Precision score: {round(self.scores['precision'], 3)}\")\n",
    "        print(f\"Recall score:    {round(self.scores['recall'], 3)}\")\n",
    "        print(f\"F1 score:        {round(self.scores['f1'], 3)}\")\n",
    "        print(f\"Wall Clock Fit:  {round(self.scores['wall_clock_fit'], 3)}\")\n",
    "        print(f\"Wall Clock Pred: {round(self.scores['wall_clock_pred'], 3)}\")\n",
    "        # Classification report\n",
    "        print(f\"\\nClassification report:\\n{self.class_report}\")\n",
    "\n",
    "        # Best score and best params\n",
    "        print(f\"Best score: {round(self.clf.best_score_, 3)}\")\n",
    "        print(\"Best params:\")\n",
    "        for param in self.clf.best_params_.items():\n",
    "            print(f\"\\t{param}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP pipeline\n",
    "mlp_pipe = Pipeline(\n",
    "    steps=[(\"scaler\", StandardScaler()), (\"model\", MLPClassifier(random_state=42))]\n",
    ")\n",
    "# GridSearchCV params\n",
    "mlp_param_grid = {\n",
    "    \"model__activation\": [\"relu\", \"tanh\", \"logistic\"],\n",
    "    \"model__max_iter\": [1000],\n",
    "    \"model__hidden_layer_sizes\": [\n",
    "        (\n",
    "            100,\n",
    "            100,\n",
    "        ),\n",
    "        (\n",
    "            200,\n",
    "            200,\n",
    "        ),\n",
    "        (\n",
    "            200,\n",
    "            200,\n",
    "            200,\n",
    "        ),\n",
    "        (\n",
    "            300,\n",
    "            300,\n",
    "        ),\n",
    "        (\n",
    "            300,\n",
    "            300,\n",
    "            300,\n",
    "        ),\n",
    "    ],\n",
    "    \"model__learning_rate\": [\"constant\", \"adaptive\"],\n",
    "    \"model__learning_rate_init\": [0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "\n",
    "# Init learners\n",
    "mlp = Learner(pipe=mlp_pipe, params=mlp_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.3min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.4min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.6min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.8min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(300, 300), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.5min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.9min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(300, 300), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.6min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(300, 300), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 2.1min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(300, 300), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.9min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(300, 300, 300), model__learning_rate=constant, model__learning_rate_init=0.01, model__max_iter=1000; total time= 2.4min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(300, 300), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 2.8min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(300, 300, 300), model__learning_rate=constant, model__learning_rate_init=0.01, model__max_iter=1000; total time= 2.4min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(300, 300, 300), model__learning_rate=constant, model__learning_rate_init=0.01, model__max_iter=1000; total time= 2.2min\n",
      "[CV] END model__activation=logistic, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.4min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(300, 300, 300), model__learning_rate=constant, model__learning_rate_init=0.01, model__max_iter=1000; total time= 2.5min\n",
      "[CV] END model__activation=logistic, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.4min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(300, 300, 300), model__learning_rate=constant, model__learning_rate_init=0.01, model__max_iter=1000; total time= 2.5min\n",
      "[CV] END model__activation=logistic, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.2min\n",
      "[CV] END model__activation=logistic, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.4min\n",
      "[CV] END model__activation=logistic, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.3min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(100, 100), model__learning_rate=constant, model__learning_rate_init=0.001, model__max_iter=1000; total time= 1.5min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(100, 100), model__learning_rate=constant, model__learning_rate_init=0.001, model__max_iter=1000; total time= 1.3min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(100, 100), model__learning_rate=constant, model__learning_rate_init=0.001, model__max_iter=1000; total time= 1.3min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(100, 100), model__learning_rate=constant, model__learning_rate_init=0.001, model__max_iter=1000; total time= 1.6min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(100, 100), model__learning_rate=constant, model__learning_rate_init=0.001, model__max_iter=1000; total time= 1.5min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(300, 300, 300), model__learning_rate=constant, model__learning_rate_init=0.1, model__max_iter=1000; total time= 2.0min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(300, 300, 300), model__learning_rate=constant, model__learning_rate_init=0.1, model__max_iter=1000; total time= 1.4min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(300, 300, 300), model__learning_rate=constant, model__learning_rate_init=0.1, model__max_iter=1000; total time= 1.9min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(300, 300, 300), model__learning_rate=constant, model__learning_rate_init=0.1, model__max_iter=1000; total time= 1.8min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.001, model__max_iter=1000; total time= 2.0min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.001, model__max_iter=1000; total time= 2.4min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.001, model__max_iter=1000; total time= 2.0min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(300, 300, 300), model__learning_rate=constant, model__learning_rate_init=0.1, model__max_iter=1000; total time= 4.9min\n",
      "[CV] END model__activation=logistic, model__hidden_layer_sizes=(100, 100), model__learning_rate=adaptive, model__learning_rate_init=0.1, model__max_iter=1000; total time=  32.3s\n",
      "[CV] END model__activation=logistic, model__hidden_layer_sizes=(100, 100), model__learning_rate=adaptive, model__learning_rate_init=0.1, model__max_iter=1000; total time=  36.5s\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.001, model__max_iter=1000; total time= 1.9min\n",
      "[CV] END model__activation=logistic, model__hidden_layer_sizes=(100, 100), model__learning_rate=adaptive, model__learning_rate_init=0.1, model__max_iter=1000; total time=  32.4s\n",
      "[CV] END model__activation=logistic, model__hidden_layer_sizes=(100, 100), model__learning_rate=adaptive, model__learning_rate_init=0.1, model__max_iter=1000; total time=  37.6s\n",
      "[CV] END model__activation=logistic, model__hidden_layer_sizes=(100, 100), model__learning_rate=adaptive, model__learning_rate_init=0.1, model__max_iter=1000; total time=  34.7s\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.001, model__max_iter=1000; total time= 1.8min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time=  58.8s\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.4min\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.4min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200, 200), model__learning_rate=constant, model__learning_rate_init=0.1, model__max_iter=1000; total time=  36.9s\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 2.3min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200, 200), model__learning_rate=constant, model__learning_rate_init=0.1, model__max_iter=1000; total time=  38.0s\n",
      "[CV] END model__activation=relu, model__hidden_layer_sizes=(200, 200), model__learning_rate=adaptive, model__learning_rate_init=0.01, model__max_iter=1000; total time= 1.3min\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200, 200), model__learning_rate=constant, model__learning_rate_init=0.1, model__max_iter=1000; total time=  30.4s\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200, 200), model__learning_rate=constant, model__learning_rate_init=0.1, model__max_iter=1000; total time=  35.0s\n",
      "[CV] END model__activation=tanh, model__hidden_layer_sizes=(200, 200, 200), model__learning_rate=constant, model__learning_rate_init=0.1, model__max_iter=1000; total time=  25.7s\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "mlp.fit_gridsearch(RandomizedSearchCV, X_train, y_train, verbose=2)\n",
    "# Get model scores\n",
    "mlp.get_scores(X_train, X_test, y_train, y_test, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################################\n",
      "RandomizedSearchCV:\t  MLPClassifier\n",
      "Train score:     0.948\n",
      "Test score:      0.852\n",
      "Accuracy score:  0.852\n",
      "Precision score: 0.647\n",
      "Recall score:    0.61\n",
      "F1 score:        0.615\n",
      "Wall Clock Fit:  100.014\n",
      "Wall Clock Pred: 0.26\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abethr1       0.33      0.25      0.29         8\n",
      "     abhori1       0.86      0.93      0.90       268\n",
      "     abythr1       0.60      0.60      0.60        30\n",
      "     afbfly1       0.00      0.00      0.00        11\n",
      "     afdfly1       1.00      0.80      0.89        46\n",
      "     afecuc1       0.93      0.92      0.92       190\n",
      "     affeag1       0.92      0.83      0.87        41\n",
      "     afgfly1       1.00      0.57      0.73        14\n",
      "     afghor1       0.86      0.86      0.86       136\n",
      "     afmdov1       0.82      0.70      0.76        84\n",
      "     afpfly1       0.70      0.79      0.74       125\n",
      "     afpkin1       0.00      0.00      0.00         1\n",
      "     afpwag1       0.84      0.93      0.88       137\n",
      "     afrgos1       0.91      0.93      0.92       104\n",
      "     afrgrp1       0.00      0.00      0.00         5\n",
      "     afrjac1       0.25      0.29      0.27         7\n",
      "     afrthr1       0.71      0.76      0.74        75\n",
      "     amesun2       0.59      0.57      0.58        53\n",
      "     augbuz1       0.54      0.54      0.54        13\n",
      "     bagwea1       0.67      0.50      0.57         4\n",
      "      barswa       0.94      0.94      0.94       859\n",
      "     bawhor2       0.73      0.88      0.80        49\n",
      "     bawman1       0.55      0.86      0.67        14\n",
      "     bcbeat1       0.86      0.88      0.87       108\n",
      "     beasun2       0.50      0.26      0.34        19\n",
      "     bkctch1       0.80      0.84      0.82       108\n",
      "     bkfruw1       0.81      0.94      0.87        49\n",
      "     blacra1       0.87      0.73      0.79        99\n",
      "     blacuc1       0.90      0.88      0.89       184\n",
      "     blakit1       0.79      0.85      0.82       246\n",
      "     blaplo1       0.89      0.94      0.91        95\n",
      "     blbpuf2       0.77      0.75      0.76       146\n",
      "     blcapa2       0.73      0.80      0.76        20\n",
      "     blfbus1       0.80      0.81      0.80        48\n",
      "     blhgon1       0.60      0.50      0.55        30\n",
      "     blhher1       0.60      0.30      0.40        10\n",
      "     blksaw1       0.00      0.00      0.00         4\n",
      "     blnmou1       0.86      0.43      0.57        14\n",
      "     blnwea1       0.40      0.75      0.52         8\n",
      "     bltapa1       1.00      0.43      0.60         7\n",
      "     bltbar1       0.91      0.71      0.80        14\n",
      "     bltori1       0.50      0.25      0.33         4\n",
      "     blwlap1       1.00      0.50      0.67         8\n",
      "     brcale1       0.17      0.25      0.20         4\n",
      "     brcsta1       0.40      0.22      0.29         9\n",
      "     brctch1       0.39      0.34      0.36        41\n",
      "     brcwea1       0.40      0.50      0.44         4\n",
      "     brican1       0.40      0.43      0.41        14\n",
      "     brobab1       0.60      0.25      0.35        12\n",
      "     broman1       0.67      0.67      0.67        43\n",
      "     brosun1       0.29      0.18      0.22        11\n",
      "     brrwhe3       0.50      0.67      0.57         3\n",
      "     brtcha1       0.67      1.00      0.80         2\n",
      "     brubru1       0.93      0.84      0.88       108\n",
      "     brwwar1       0.82      0.79      0.81        53\n",
      "     bswdov1       0.42      0.53      0.47        38\n",
      "     btweye2       0.77      0.76      0.76        91\n",
      "     bubwar2       0.24      0.27      0.25        15\n",
      "     butapa1       0.83      0.77      0.80        13\n",
      "     cabgre1       0.39      0.73      0.51        30\n",
      "     carcha1       0.86      0.95      0.90       377\n",
      "     carwoo1       0.80      0.82      0.81        34\n",
      "      categr       0.83      0.72      0.77       153\n",
      "     ccbeat1       0.89      0.80      0.84        10\n",
      "     chespa1       0.80      0.33      0.47        12\n",
      "     chewea1       0.21      0.33      0.26         9\n",
      "     chibat1       0.62      0.74      0.68        98\n",
      "     chtapa3       0.47      0.53      0.50        15\n",
      "     chucis1       0.60      0.43      0.50         7\n",
      "     cibwar1       0.84      0.78      0.81       138\n",
      "     cohmar1       0.94      0.94      0.94       757\n",
      "     colsun2       0.93      0.93      0.93       400\n",
      "     combul2       0.83      0.89      0.86       549\n",
      "     combuz1       0.92      0.94      0.93       803\n",
      "      comsan       0.98      0.93      0.95       553\n",
      "     crefra2       0.33      0.50      0.40         2\n",
      "     crheag1       0.80      0.94      0.87        48\n",
      "     crohor1       0.94      0.94      0.94       116\n",
      "     darbar1       0.95      0.97      0.96        58\n",
      "     darter3       0.43      0.43      0.43         7\n",
      "     didcuc1       0.85      0.82      0.83        89\n",
      "     dotbar1       0.00      0.00      0.00         2\n",
      "     dutdov1       0.65      0.81      0.72        16\n",
      "     easmog1       0.00      0.00      0.00         1\n",
      "     eaywag1       0.87      0.92      0.90       579\n",
      "     edcsun3       0.50      0.30      0.37        10\n",
      "      egygoo       0.89      0.84      0.86       230\n",
      "     equaka1       0.40      0.50      0.44        16\n",
      "     eswdov1       0.78      0.75      0.76       138\n",
      "     eubeat1       0.97      0.97      0.97      1028\n",
      "     fatrav1       1.00      0.73      0.84        11\n",
      "     fatwid1       0.50      0.40      0.44         5\n",
      "     fislov1       0.25      0.25      0.25         4\n",
      "     fotdro5       0.62      0.73      0.67       161\n",
      "     gabgos2       0.80      0.89      0.84        66\n",
      "      gargan       0.55      0.67      0.60        87\n",
      "     gbesta1       0.62      0.63      0.62        46\n",
      "     gnbcam2       0.86      0.80      0.83       467\n",
      "     gnhsun1       0.47      0.47      0.47        15\n",
      "     gobbun1       0.67      0.63      0.65        93\n",
      "     gobsta5       0.67      0.50      0.57         4\n",
      "     gobwea1       0.77      0.62      0.69        16\n",
      "     golher1       0.00      0.00      0.00         1\n",
      "     grbcam1       0.73      0.85      0.79       186\n",
      "     grccra1       0.14      0.25      0.18         4\n",
      "      grecor       0.83      0.81      0.82       212\n",
      "      greegr       0.73      0.85      0.78       195\n",
      "     grewoo2       0.81      0.88      0.84       178\n",
      "     grwpyt1       0.45      0.50      0.48        10\n",
      "     gryapa1       0.85      0.95      0.90        55\n",
      "     grywrw1       1.00      0.78      0.88        18\n",
      "     gybfis1       0.50      0.14      0.22         7\n",
      "     gycwar3       0.84      0.82      0.83        83\n",
      "     gyhbus1       0.90      0.79      0.84       160\n",
      "     gyhkin1       0.53      0.44      0.48        18\n",
      "     gyhneg1       0.40      0.40      0.40        10\n",
      "     gyhspa1       0.85      0.78      0.81       114\n",
      "     gytbar1       0.60      0.38      0.46         8\n",
      "     hadibi1       0.88      0.91      0.89       170\n",
      "     hamerk1       0.74      0.82      0.78        17\n",
      "     hartur1       0.76      0.93      0.84        14\n",
      "      helgui       0.86      0.89      0.87       134\n",
      "     hipbab1       0.88      0.64      0.74        11\n",
      "      hoopoe       0.92      0.93      0.93      1261\n",
      "     huncis1       0.00      0.00      0.00         6\n",
      "     hunsun2       0.64      0.37      0.47        19\n",
      "     joygre1       0.38      0.45      0.42        11\n",
      "     kerspa2       0.79      0.87      0.83        53\n",
      "     klacuc1       0.68      0.57      0.62        60\n",
      "     kvbsun1       0.41      0.50      0.45        14\n",
      "     laudov1       0.87      0.92      0.89       260\n",
      "      lawgol       0.94      0.91      0.92       159\n",
      "     lesmaw1       0.00      0.00      0.00         4\n",
      "     lessts1       0.55      0.60      0.57        30\n",
      "     libeat1       0.29      0.25      0.27         8\n",
      "      litegr       0.86      0.77      0.81       264\n",
      "     litswi1       0.88      0.94      0.91       147\n",
      "     litwea1       0.00      0.00      0.00         3\n",
      "     loceag1       0.27      0.30      0.29        10\n",
      "     lotcor1       0.50      0.33      0.40         3\n",
      "     lotlap1       0.00      0.00      0.00         2\n",
      "     luebus1       0.57      0.36      0.44        11\n",
      "     mabeat1       0.79      0.65      0.71        17\n",
      "     macshr1       1.00      0.50      0.67         6\n",
      "     malkin1       0.50      0.33      0.40         6\n",
      "     marsto1       0.76      0.76      0.76        17\n",
      "     marsun2       0.63      0.59      0.61        37\n",
      "     mcptit1       1.00      0.50      0.67         4\n",
      "     meypar1       0.40      0.25      0.31         8\n",
      "     moccha1       0.43      0.38      0.41        26\n",
      "     mouwag1       0.43      0.59      0.50        17\n",
      "     ndcsun2       0.50      0.50      0.50        12\n",
      "     nobfly1       0.00      0.00      0.00         8\n",
      "     norbro1       1.00      0.12      0.22         8\n",
      "     norcro1       0.59      0.62      0.61        16\n",
      "     norfis1       0.40      0.40      0.40        10\n",
      "     norpuf1       0.65      0.55      0.59        20\n",
      "     nubwoo1       1.00      0.62      0.77        16\n",
      "     pabspa1       0.50      0.82      0.62        11\n",
      "     palfly2       0.00      0.00      0.00         2\n",
      "     palpri1       1.00      0.75      0.86         8\n",
      "     piecro1       0.76      0.84      0.80       117\n",
      "     piekin1       0.85      0.65      0.74        71\n",
      "      pitwhy       0.92      0.69      0.79        64\n",
      "     purgre2       0.43      0.55      0.48        11\n",
      "     pygbat1       0.50      0.33      0.40         6\n",
      "     quailf1       0.67      0.44      0.53         9\n",
      "     ratcis1       0.76      0.81      0.78       167\n",
      "     raybar1       1.00      1.00      1.00         2\n",
      "     rbsrob1       0.85      0.91      0.88       586\n",
      "     rebfir2       0.52      0.38      0.44        34\n",
      "     rebhor1       0.79      0.85      0.81        13\n",
      "     reboxp1       0.81      0.81      0.81        26\n",
      "      reccor       0.72      0.78      0.75        69\n",
      "     reccuc1       0.96      0.95      0.96       303\n",
      "     reedov1       0.74      0.86      0.79       125\n",
      "     refbar2       0.33      0.25      0.29         4\n",
      "     refcro1       0.86      0.94      0.90        47\n",
      "     reftin1       0.94      0.91      0.92       128\n",
      "     refwar2       0.29      0.50      0.36         4\n",
      "     rehblu1       0.00      0.00      0.00         1\n",
      "     rehwea1       0.67      0.36      0.47        11\n",
      "     reisee2       0.67      0.20      0.31        10\n",
      "     rerswa1       0.95      0.89      0.92       333\n",
      "     rewsta1       0.62      0.62      0.62        16\n",
      "      rindov       0.92      0.88      0.90       243\n",
      "     rocmar2       0.87      0.62      0.72        21\n",
      "     rostur1       0.33      0.29      0.31        14\n",
      "     ruegls1       0.19      0.26      0.22        27\n",
      "     rufcha2       0.46      0.55      0.50        11\n",
      "     sacibi2       0.25      0.57      0.35         7\n",
      "     sccsun2       0.82      0.77      0.79       115\n",
      "     scrcha1       0.65      0.52      0.58        29\n",
      "     scthon1       0.63      0.86      0.73        14\n",
      "     shesta1       0.88      1.00      0.93         7\n",
      "     sichor1       0.89      0.73      0.80        70\n",
      "     sincis1       0.78      0.76      0.77        80\n",
      "     slbgre1       0.33      0.14      0.20         7\n",
      "     slcbou1       0.77      0.67      0.71        54\n",
      "     sltnig1       0.73      0.83      0.78        23\n",
      "     sobfly1       0.75      0.30      0.43        10\n",
      "     somgre1       0.94      0.93      0.94       408\n",
      "     somtit4       0.75      0.33      0.46         9\n",
      "     soucit1       0.75      0.43      0.55         7\n",
      "     soufis1       0.60      0.26      0.36        23\n",
      "     spemou2       0.67      0.76      0.71        37\n",
      "     spepig1       0.97      0.84      0.90        81\n",
      "     spewea1       0.00      0.00      0.00         5\n",
      "     spfbar1       0.70      0.33      0.45        21\n",
      "     spfwea1       0.54      0.54      0.54        13\n",
      "     spmthr1       0.52      0.41      0.46        66\n",
      "     spwlap1       0.95      0.94      0.94       111\n",
      "     squher1       0.73      0.58      0.65        19\n",
      "      strher       0.77      0.67      0.72        36\n",
      "     strsee1       0.67      0.78      0.72        23\n",
      "     stusta1       0.56      0.50      0.53        10\n",
      "     subbus1       0.81      0.88      0.84       122\n",
      "     supsta1       0.74      0.62      0.68        40\n",
      "     tacsun1       0.21      0.27      0.24        11\n",
      "     tafpri1       0.86      0.81      0.84       204\n",
      "     tamdov1       0.92      0.91      0.91       250\n",
      "     thrnig1       0.94      0.95      0.95      1265\n",
      "     trobou1       0.81      0.84      0.82       165\n",
      "     varsun2       0.81      0.84      0.82       150\n",
      "     vibsta2       0.70      0.76      0.73        21\n",
      "     vilwea1       0.75      0.79      0.77       145\n",
      "     vimwea1       0.55      0.35      0.43        17\n",
      "     walsta1       0.43      0.43      0.43        14\n",
      "     wbgbir1       0.87      0.87      0.87        38\n",
      "     wbrcha2       0.83      0.76      0.80       230\n",
      "     wbswea1       0.71      0.64      0.67        92\n",
      "     wfbeat1       0.90      0.90      0.90        42\n",
      "     whbcan1       0.11      0.33      0.17         3\n",
      "     whbcou1       0.69      0.72      0.70        61\n",
      "     whbcro2       0.50      0.50      0.50         8\n",
      "     whbtit5       0.33      0.50      0.40        10\n",
      "     whbwea1       0.56      0.62      0.59         8\n",
      "     whbwhe3       0.76      0.86      0.81        29\n",
      "     whcpri2       0.71      0.77      0.74        22\n",
      "     whctur2       0.00      0.00      0.00         1\n",
      "     wheslf1       0.29      0.29      0.29         7\n",
      "     whhsaw1       0.00      0.00      0.00         1\n",
      "     whihel1       0.20      0.33      0.25         3\n",
      "     whrshr1       0.70      0.58      0.64        12\n",
      "     witswa1       0.18      0.75      0.29         4\n",
      "      wlwwar       0.94      0.92      0.93       887\n",
      "     wookin1       0.96      0.83      0.89       113\n",
      "      woosan       0.95      0.93      0.94       509\n",
      "     wtbeat1       0.62      0.45      0.53        11\n",
      "     yebapa1       0.90      0.80      0.85       178\n",
      "     yebbar1       0.91      0.85      0.88        71\n",
      "     yebduc1       0.80      0.50      0.62         8\n",
      "     yebere1       0.78      0.41      0.54        17\n",
      "     yebgre1       0.78      0.72      0.75        29\n",
      "     yebsto1       1.00      1.00      1.00         6\n",
      "     yeccan1       0.55      0.86      0.67         7\n",
      "      yefcan       0.60      0.70      0.65       110\n",
      "     yelbis1       1.00      0.69      0.82        13\n",
      "     yenspu1       0.67      0.43      0.52        14\n",
      "     yertin1       0.95      0.93      0.94       296\n",
      "     yesbar1       0.71      0.62      0.67        16\n",
      "     yespet1       0.62      0.71      0.67         7\n",
      "     yetgre1       0.17      0.20      0.18         5\n",
      "     yewgre1       0.80      0.80      0.80       157\n",
      "\n",
      "    accuracy                           0.85     24582\n",
      "   macro avg       0.65      0.61      0.61     24582\n",
      "weighted avg       0.85      0.85      0.85     24582\n",
      "\n",
      "Best score: nan\n",
      "Best params:\n",
      "\t('model__max_iter', 1000)\n",
      "\t('model__learning_rate_init', 0.01)\n",
      "\t('model__learning_rate', 'adaptive')\n",
      "\t('model__hidden_layer_sizes', (200, 200))\n",
      "\t('model__activation', 'tanh')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print scores\n",
    "mlp.evaluate_learner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(200, 200),\n",
       "                               learning_rate=&#x27;adaptive&#x27;,\n",
       "                               learning_rate_init=0.01, max_iter=1000,\n",
       "                               random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;model&#x27;,\n",
       "                 MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(200, 200),\n",
       "                               learning_rate=&#x27;adaptive&#x27;,\n",
       "                               learning_rate_init=0.01, max_iter=1000,\n",
       "                               random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(200, 200),\n",
       "              learning_rate=&#x27;adaptive&#x27;, learning_rate_init=0.01, max_iter=1000,\n",
       "              random_state=42)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('model',\n",
       "                 MLPClassifier(activation='tanh', hidden_layer_sizes=(200, 200),\n",
       "                               learning_rate='adaptive',\n",
       "                               learning_rate_init=0.01, max_iter=1000,\n",
       "                               random_state=42))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLPClassifier best estimator\n",
    "mlp.clf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(truth, preds):\n",
    "    print(\"Accuracy:\", round(accuracy_score(truth, preds), 3))\n",
    "    print(\"Precision:\", round(precision_score(truth, preds, average=\"macro\"), 3))\n",
    "    print(\"Recall:\", round(recall_score(truth, preds, average=\"macro\"), 3))\n",
    "    print(\"F1 Score:\", round(f1_score(truth, preds, average=\"macro\"), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.852\n",
      "Precision: 0.647\n",
      "Recall: 0.61\n",
      "F1 Score: 0.615\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "cv_model = mlp.clf.best_estimator_\n",
    "model_eval(y_test, cv_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Write to pickle file\n",
    "pickle.dump(\n",
    "    cv_model,\n",
    "    Path(\"../data/models/baseline/mlp-v1.pkl\").open(\"wb\"),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    np.stack(data[\"embedding\"]),\n",
    "    data[\"species\"],\n",
    "    test_size=0.33,\n",
    "    stratify=data[\"species\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform the target with label encoder\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "# XGBoost pipeline\n",
    "xgb_pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", XGBClassifier(seed=42)),\n",
    "    ]\n",
    ")\n",
    "# GridSearchCV params\n",
    "xgb_param_grid = {\n",
    "    \"model__objective\": [\"multi:softmax\"],\n",
    "    \"model__n_estimators\": [400],  # np.arange(50, 200, 50),\n",
    "    \"model__max_depth\": [\n",
    "        3,\n",
    "        4,\n",
    "        5,\n",
    "    ],  # np.arange(3, 10),\n",
    "    # \"model__min_child_weight\": np.arange(1, 6),\n",
    "    # \"model__gamma\": np.linspace(0, 0.6, 5),\n",
    "    # \"model__subsample\": np.linspace(0.5, 1.0, 6),\n",
    "    # \"model__colsample_bytree\": np.linspace(0.5, 1.0, 6),\n",
    "}\n",
    "\n",
    "\n",
    "# Init learners\n",
    "xgb = Learner(pipe=xgb_pipe, params=xgb_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "xgb.fit_gridsearch(RandomizedSearchCV, X_train, y_train_enc, verbose=2)\n",
    "# Get model scores\n",
    "# xgb.get_scores(X_train, X_test, y_train_enc, y_test_enc, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "xgb = XGBClassifier(n_estimators=400, learning_rate=0.1, max_depth=3)\n",
    "xgb.fit(X=X_train, y=y_train_enc)\n",
    "print(\"Fit time : \", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = xgb.predict(X_test)\n",
    "y_preds = le.inverse_transform(y_preds)\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_preds), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_bird_classes(y_train):\n",
    "    class_counts = y_train.value_counts()\n",
    "    fig, ax = plt.subplots(figsize=(6.4, 4.8), dpi=200)\n",
    "    ax.margins(x=0.01, y=0.1)  # No margins on x and y-axis\n",
    "    x = np.arange(len(class_counts))\n",
    "    width = 0.7\n",
    "    ax.bar(class_counts.index, class_counts.values, width=width, color=\"tab:blue\")\n",
    "    ax.set_title(\n",
    "        f\"Total count of bird species for each class\", weight=\"bold\", fontsize=16\n",
    "    )\n",
    "    ax.set_xlabel(\"Bird Species\")\n",
    "    ax.set_ylabel(\"Total count\")\n",
    "    ax.set_xticks([])\n",
    "    ax.grid(color=\"blue\", linestyle=\"--\", linewidth=1, alpha=0.2)\n",
    "    for spine in [\"top\", \"right\", \"bottom\", \"left\"]:\n",
    "        ax.spines[spine].set_visible(False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
